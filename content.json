{"meta":{"title":"西左Log","subtitle":"","description":"自是者不彰。","author":"西左","url":"https://dangyoo.github.io","root":"/"},"pages":[{"title":"分类","date":"2023-08-12T13:31:27.000Z","updated":"2023-08-12T13:33:58.957Z","comments":true,"path":"categories/index.html","permalink":"https://dangyoo.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2023-08-28T05:44:12.766Z","updated":"2023-08-28T05:44:12.766Z","comments":true,"path":"images/next_config.json","permalink":"https://dangyoo.github.io/images/next_config.json","excerpt":"","text":"{\"override\":false,\"reminder\":false,\"cache\":{\"enable\":true},\"minify\":false,\"custom_file_path\":null,\"favicon\":{\"small\":\"/images/favicon-16x16-next.png\",\"medium\":\"/images/favicon-32x32-next.png\",\"apple_touch_icon\":\"/images/apple-touch-icon-next.png\",\"safari_pinned_tab\":\"/images/logo.svg\"},\"language_switcher\":false,\"footer\":{\"icon\":{\"name\":\"fa fa-heart\",\"animated\":false,\"color\":\"#ff0000\"},\"copyright\":null,\"powered\":true,\"beian\":{\"enable\":false,\"icp\":null,\"gongan_id\":null,\"gongan_num\":null,\"gongan_icon_url\":null}},\"creative_commons\":{\"license\":\"by-nc-sa\",\"sidebar\":false,\"post\":false,\"language\":\"zh-CN\"},\"scheme\":\"Gemini\",\"darkmode\":false,\"menu\":{\"home\":\"/ || fa fa-home\",\"tags\":\"/tags/ || fa fa-tags\",\"categories\":\"/categories/ || fa fa-th\",\"archives\":\"/archives/ || fa fa-archive\"},\"menu_settings\":{\"icons\":true,\"badges\":false},\"sidebar\":{\"position\":\"left\",\"display\":\"post\",\"padding\":18,\"offset\":12,\"onmobile\":false},\"avatar\":{\"url\":\"/images/avatar.jpeg\",\"rounded\":false,\"rotated\":false},\"site_state\":true,\"social\":{\"GitHub\":\"https://github.com/dangyoo || fab fa-github\"},\"social_icons\":{\"enable\":true,\"icons_only\":true,\"transition\":false},\"links_settings\":{\"icon\":\"fa fa-link\",\"title\":\"Links\",\"layout\":\"block\"},\"links\":null,\"toc\":{\"enable\":true,\"number\":true,\"wrap\":false,\"expand_all\":false,\"max_depth\":6},\"chat\":{\"enable\":false,\"icon\":\"fa fa-comment\",\"text\":\"Chat\"},\"excerpt_description\":true,\"read_more_btn\":true,\"post_meta\":{\"item_text\":true,\"created_at\":true,\"updated_at\":{\"enable\":false,\"another_day\":true},\"categories\":true},\"symbols_count_time\":{\"separated_meta\":true,\"item_text_post\":true,\"item_text_total\":false},\"tag_icon\":false,\"reward_settings\":{\"enable\":false,\"animation\":false},\"reward\":null,\"follow_me\":null,\"related_posts\":{\"enable\":false,\"title\":null,\"display_in_home\":false,\"params\":{\"maxCount\":5}},\"post_edit\":{\"enable\":false,\"url\":\"https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name\"},\"post_navigation\":\"left\",\"tagcloud\":{\"min\":12,\"max\":30,\"start\":\"#ccc\",\"end\":\"#111\",\"amount\":200},\"calendar\":{\"calendar_id\":\"\",\"api_key\":\"\",\"orderBy\":\"startTime\",\"offsetMax\":24,\"offsetMin\":4,\"showDeleted\":false,\"singleEvents\":true,\"maxResults\":250},\"text_align\":{\"desktop\":\"justify\",\"mobile\":\"justify\"},\"mobile_layout_economy\":false,\"android_chrome_color\":\"#222\",\"custom_logo\":null,\"codeblock\":{\"highlight_theme\":\"normal\",\"copy_button\":{\"enable\":false,\"show_result\":false,\"style\":null}},\"back2top\":{\"enable\":true,\"sidebar\":false,\"scrollpercent\":true},\"reading_progress\":{\"enable\":false,\"position\":\"top\",\"color\":\"#37c6c0\",\"height\":\"3px\"},\"bookmark\":{\"enable\":false,\"color\":\"#222\",\"save\":\"auto\"},\"github_banner\":{\"enable\":false,\"permalink\":\"https://github.com/dangyoo\",\"title\":\"Follow me on GitHub\"},\"font\":{\"enable\":false,\"host\":null,\"global\":{\"external\":true,\"family\":\"Lato\",\"size\":null},\"title\":{\"external\":true,\"family\":null,\"size\":null},\"headings\":{\"external\":true,\"family\":null,\"size\":null},\"posts\":{\"external\":true,\"family\":null},\"codes\":{\"external\":true,\"family\":null}},\"disable_baidu_transformation\":false,\"index_with_subtitle\":false,\"exturl\":false,\"google_site_verification\":null,\"bing_site_verification\":null,\"yandex_site_verification\":null,\"baidu_site_verification\":null,\"baidu_push\":false,\"math\":{\"per_page\":true,\"mathjax\":{\"enable\":false,\"mhchem\":false},\"katex\":{\"enable\":false,\"copy_tex\":false}},\"pjax\":false,\"fancybox\":true,\"mediumzoom\":false,\"lazyload\":false,\"pangu\":false,\"quicklink\":{\"enable\":false,\"home\":false,\"archive\":false,\"delay\":true,\"timeout\":3000,\"priority\":true,\"ignores\":null},\"comments\":{\"style\":\"tabs\",\"active\":null,\"storage\":true,\"lazyload\":false,\"nav\":null},\"disqus\":{\"enable\":false,\"shortname\":null,\"count\":true},\"disqusjs\":{\"enable\":false,\"api\":null,\"apikey\":null,\"shortname\":null},\"changyan\":{\"enable\":false,\"appid\":null,\"appkey\":null},\"valine\":{\"enable\":false,\"appid\":null,\"appkey\":null,\"notify\":false,\"verify\":false,\"placeholder\":\"Just go go\",\"avatar\":\"mm\",\"guest_info\":\"nick,mail,link\",\"pageSize\":10,\"language\":null,\"visitor\":false,\"comment_count\":true,\"recordIP\":false,\"serverURLs\":null},\"livere_uid\":null,\"gitalk\":{\"enable\":false,\"github_id\":null,\"repo\":null,\"client_id\":null,\"client_secret\":null,\"admin_user\":null,\"distraction_free_mode\":true,\"language\":\"zh-CN\"},\"rating\":{\"enable\":false,\"id\":null,\"color\":\"fc6423\"},\"add_this_id\":null,\"google_analytics\":{\"tracking_id\":null,\"only_pageview\":false},\"baidu_analytics\":\"15db7c636ac845c8c8fe0ca45dac6e54\",\"growingio_analytics\":null,\"cnzz_siteid\":null,\"leancloud_visitors\":{\"enable\":false,\"app_id\":null,\"app_key\":null,\"server_url\":null,\"security\":true},\"firestore\":{\"enable\":false,\"collection\":\"articles\",\"apiKey\":null,\"projectId\":null},\"busuanzi_count\":{\"enable\":false,\"total_visitors\":true,\"total_visitors_icon\":\"fa fa-user\",\"total_views\":true,\"total_views_icon\":\"fa fa-eye\",\"post_views\":true,\"post_views_icon\":\"fa fa-eye\"},\"algolia_search\":{\"enable\":false,\"hits\":{\"per_page\":10},\"labels\":{\"input_placeholder\":\"Search for Posts\",\"hits_empty\":\"We didn't find any results for the search: ${query}\",\"hits_stats\":\"${hits} results found in ${time} ms\"}},\"local_search\":{\"enable\":false,\"trigger\":\"auto\",\"top_n_per_article\":1,\"unescape\":false,\"preload\":false},\"swiftype_key\":null,\"chatra\":{\"enable\":false,\"async\":true,\"id\":null},\"tidio\":{\"enable\":false,\"key\":null},\"note\":{\"style\":\"simple\",\"icons\":false,\"light_bg_offset\":0},\"tabs\":{\"transition\":{\"tabs\":false,\"labels\":true}},\"pdf\":{\"enable\":false,\"height\":\"500px\"},\"mermaid\":{\"enable\":false,\"theme\":\"forest\"},\"motion\":{\"enable\":true,\"async\":false,\"transition\":{\"post_block\":\"fadeIn\",\"post_header\":\"slideDownIn\",\"post_body\":\"slideDownIn\",\"coll_header\":\"slideLeftIn\",\"sidebar\":\"slideUpIn\"}},\"pace\":{\"enable\":false,\"theme\":\"minimal\"},\"three\":{\"enable\":false,\"three_waves\":false,\"canvas_lines\":false,\"canvas_sphere\":false},\"canvas_ribbon\":{\"enable\":false,\"size\":300,\"alpha\":0.6,\"zIndex\":-1},\"vendors\":{\"_internal\":\"lib\",\"anime\":null,\"fontawesome\":null,\"mathjax\":null,\"katex\":null,\"copy_tex_js\":null,\"copy_tex_css\":null,\"pjax\":null,\"jquery\":null,\"fancybox\":null,\"fancybox_css\":null,\"mediumzoom\":null,\"lazyload\":null,\"pangu\":null,\"quicklink\":null,\"disqusjs_js\":null,\"disqusjs_css\":null,\"valine\":null,\"gitalk_js\":null,\"gitalk_css\":null,\"algolia_search\":null,\"instant_search\":null,\"mermaid\":null,\"velocity\":null,\"velocity_ui\":null,\"pace\":null,\"pace_css\":null,\"three\":null,\"three_waves\":null,\"canvas_lines\":null,\"canvas_sphere\":null,\"canvas_ribbon\":null},\"css\":\"css\",\"js\":\"js\",\"images\":\"images\"}"},{"title":"标签","date":"2023-08-28T05:41:04.000Z","updated":"2023-08-28T05:41:37.557Z","comments":true,"path":"tags/index.html","permalink":"https://dangyoo.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Git","slug":"Git","date":"2023-09-02T08:06:03.000Z","updated":"2024-03-31T08:32:16.329Z","comments":true,"path":"2023/09/02/Git/","link":"","permalink":"https://dangyoo.github.io/2023/09/02/Git/","excerpt":"闲言碎语之前的工作中基本没有用到过Git，所有代码都是通过SaaS平台编辑管理，在上家T司本来想着后续推动一下用Git管理但是没机会，好在现在这家NT司就是用Git来管理项目的。 常用的操作比如博客管理就是用到了add、commit、push，至于里边的公钥配置啥的原理以及分支管理都搞得不太明白，刚好有机会搞一下","text":"闲言碎语之前的工作中基本没有用到过Git，所有代码都是通过SaaS平台编辑管理，在上家T司本来想着后续推动一下用Git管理但是没机会，好在现在这家NT司就是用Git来管理项目的。 常用的操作比如博客管理就是用到了add、commit、push，至于里边的公钥配置啥的原理以及分支管理都搞得不太明白，刚好有机会搞一下 Git实际工作中会遇到关于文件备份、代码还原、协同开发、追溯代码问题和负责人等场景，之前的解决方案是类似于SVN\\CVS之类的集中式版本控制工具 集中式版本控制工具会将版本库集中存放在中央服务器中，team里每个人工作时从中央服务器下载代码，必须联网才能工作，个人修改后提交到中央版本库 比集中式更进一步的是分布式版本控制工具，即Git，它没有中央服务器的概念，每个人的电脑上都是一个完整的版本库，无需联网就能工作，多人协作时只需要将各自的修改推送给对方，就能互相看到对方的修改 日常经常会有一个远程仓库来托管作为最终线上部署的内容，开发人员从远程仓库拉取到本地进行开发，最终再合并到远程仓库上 工作流程 Clone - 从远程仓库中克隆代码到本地仓库 Checkout - 从本地仓库中检出一个分支然后进行修订 Add - 在提交前现将代码提交到暂存区 Commit - 提交到本地仓库，本地仓库中保存修改的各个历史版本 Fetch - 从远程仓库抓取到本地仓库，不进行任何的合并动作 Pull - 从远程仓库拉取到本地仓库，自动进行合并Merge，然后放到工作区，相当于Fetch+Merge Push - 从本地仓库到远程仓库 常用命令基本环境配置123456789101112git config --global user.name &quot;username&quot; # 设置git config --global user.email &quot;no@thanks.com&quot;git config --global user.name # 查看git config --global user.emailgit log --pretty=oneline --all --graph --abbrev-commit # 输出提交日志# 解决Git Bash中文乱码问题git config --global core.quotepath false# 在$&#123;git_home&#125;/etc/bash 的.bashrc文件中加入# export LANG=&quot;zh_CN.UTF-8&quot;# export LC_ALL=&quot;zh_CN.UTF-8&quot; 本地仓库123# 在任意一个文件夹下git init # 初始化当前目录为一个git仓库# 执行后文件夹下出现.git文件 文件的基本状态Git工作目录（本地仓库所在目录）下对于文件的修改（增加、删除、更新）并执行Git命令的过程中，文件会处于不同的状态： untracked - 新创建的文件，位于工作区，处于未跟踪状态 staged - untracked状态的文件执行git add，则变更为已暂存状态，即将文件提交到了暂存区 commit - staged状态的文件执行git commit，则变更为已提交状态，即将文件提交到了本地仓库 unstaged - 修改已有的文件，文件位于工作区，处于未暂存状态 基础指令12345678910111213141516171819git status # 查看文件状态git add . # 将工作区文件存放到暂存区(to be committed)git commit -m &quot;&quot; # 将暂存区文件提交到本地仓库git log # 查看提交日志git log --all # 显示所有分支git log --pretty=oneline # 将提交信息显示为一行git log --abbrev-commit # 简化commitidgit log --graph # 图形化呈现git reset --hard &#123;commitID&#125; # 版本回退待commitid对应的版本git reflog # 查看已经删除的提交记录# 忽略文件.gitignore# *.a # 所有.a结尾文件都不用Git管理# !lib.a # 除了lib.a这个文件外的.a结尾文件不用Git管理# /TODO # 只忽略当前目录下的TODO文件夹，不忽略子文件# build/ # 忽略所有build文件夹下的文件# doc/*.txt # 忽略doc文件夹下的.txt结尾文件# doc/**/*.pdf # 忽略doc文件夹下及所有子文件的.pdf结尾文件 分支指令1234567git branch # 查看分支git branch &#123;branchname&#125; # 新建分支branchnamegit checkout &#123;branchname&#125; # 切换到branchname分支git checkout -b &#123;branchname&#125; # 新建分支并切换到新分支branchnamegit merge &#123;branchname&#125; # 将分支branchname合并到当前分支git branch -d &#123;branchname&#125; # 删除分支branchnamegit branch -D &#123;branchname&#125; # 强制删除没有merge的分支branchname 常见开发规范master分支 - 线上主分支 develop分支 - 从master创建的分支，一般作为开发部门的主要分支，如果没有其他并行开发不同期上线要求，都可以在此版本上进行开发，所有人的阶段开发完成后，先合并到develop分支，再合并到master分支上线 feature分支 - 从develop创建的分支，一般是同期并行开发但不同期上线时创建的分支，分支上的研发任务完成后合并到develop分支 hotfix分支 - 从master创建的分支，一般作为线上BUG修复使用，修复完成后需要合并到master、develop分支 可能还有其他如test分支（用于代码测试）、pre分支（用于预发布测试）等 托管服务又称为远程仓库，常见有Github、码云Gitee、GitLab，其中Github和Gitee为国外国内的SaaS服务，而GitLab是一个开源项目，可以用于Git私服的搭建 本地仓库连接远程仓库时，可以使用用户名密码方式，也可以使用SSH密钥认证方式（常用） 123456789101112131415161718ssh-keygen -t rsa # 生成客户端SSH密钥~/.ssh/id_rsa.pub # 公钥# 将公钥上传到远程仓库xx@xx.comssh -T xx@xx.com # 验证是否配置成功git remote add origin xx@xx.com:xx/xx.git # 添加一个远程仓库git remote # 查看已经配置的远程仓库git push origin master:master # 将本地仓库master分支推送到远程仓库origin的master分支# 如果远程分支和本地分支名称相同，可以只写一个git push --set-upstream origin master:master # 推送并关联分支git push # 已经建立关联的分支可以省略git push -f # 强制覆盖git branch -vv # 查看本地分支详情（可显示与远程仓库分支的绑定关系）git clone xx@xx.com:xx/xx.git # 克隆远程仓库到本地git fetch &#123;remotename&#125;&#123;branchname&#125; # 抓取远程分支到本地但并不进行合并git pull &#123;remotename&#125;&#123;branchname&#125; # 抓取远程分支到本地并和当前分支合并 SSHSecure Shell，安全外壳，是一种网络安全协议，通过加密和认证机制实现安全的访问和文件传输等业务。 传统的远程登录和文件传输方式，如Telnet、FTP，使用明文传输数据，存在很多安全隐患。 TelnetTelnet是一种应用层协议，使用于互联网及局域网中，使用虚拟终端的形式，提供双向、以文字字符串为主的命令行接口交互功能。属于TCP&#x2F;IP协议族的其中之一，是互联网远程登录服务的标准协议和主要方式，常用于服务器的远程控制，可供用户在本地主机运行远程主机上的工作。 FTPFile Transfer Protocol，文件传输协议，是在计算机网络的用户端和服务器之间传输文件的应用层协议。FTP的目标时提供文件的共享性，而非直接使用远程计算机，使用TCP传输而非UDP TCPTransmission Control Protocol，传输控制协议，定义了两台计算机之间进行可靠的传输而交换的数据和确认信息的格式，以及计算机为了确保数据的正确到达而采取的措施。协议规定了TCP软件怎样识别给定计算机上的多个目的进程如何对分组重复这类差错进行恢复。协议还规定了两台计算机如何初始化一个TCP数据流传输以及如何结束这一传输。 UDPUser Datagram Protocol，用户数据报协议，是一个简单的面向数据报的传输层协议。提供的是非面向连续的、不可靠的数据流传输。UDP不提供可靠性，也不提供报文到达确认、排序以及流量控制等功能，它知识吧应用程序传给IP层的数据报发送出去，但不能保证它们到达目的地。因此报文可能会丢失、重复以及乱序，但由于UDP在传输数据报前不用在客户和服务器之间建立连接，且没有超时重发等机制，故而传输速度很快。常见的是视频、音乐应用应用的基本都是UDP SSH工作方式SSH由服务器和客户端组成，为了建立安全的SSH通道，双方需要先建立TCP连接，然后协商使用的版本号和各类算法，并生成相同的会话密钥用于后续的对称加密。在完成用户认证后，双方即可建立会话进行数据交互，主要的工作流程包括以下几个阶段： 连接建立SSH依赖端口进行通信，在未建立SSH连接时，SSH服务器会在指定端口（22）侦听连接请求，SSH客户端向服务器该指定端口发起连接请求后，双方建立一个TCP连接，后续通过该端口进行通信 版本协商SSH协议目前存在SSH1.x和2.0版本，2.0协议相比于1.x来说，在结构上做了扩展，可以支持更多的认证方法和密钥交换方法，同时提高了服务能力 算法协商SSH工作过程中需要使用多种类型的算法，包括用于产生会话密钥的密钥交换算法、用于数据信息加密的对称加密算法、用于进行数字签名和认证的公钥算法那、用于数据完整性保护的HMAC算法等 密钥交换SSH服务器和客户端通过密钥交换算法，动态生成共享会话密钥和会话ID，建立加密通道。会话密钥主要用于后续数据传输的加密，会话ID用于在认证过程中标识该SSH连接。 由于SSH服务器和客户端需要持有相同的会话密钥用于后续的对称加密，为了保证密钥交换的安全性，SSH使用一种安全的方式生成会话密钥： SSH服务器生成素数G、P、服务器私钥b，并计算得到服务器公钥y&#x3D;(G^b)%P SSH服务器将素数G、P、服务器公钥y发送给SSH客户端 SSH客户端生成客户端私钥a，计算得到客户端公钥x&#x3D;(G^a)%P SSH客户端将公钥x发送给SSH服务器 SSH服务器计算得到对称密钥K&#x3D;(x^b)%P，SSH客户端计算得到对称密钥K&#x3D;(y^a)%P，数学定律可以保证两者相同（RSA算法，欧拉定理） 用户认证SSH客户端向SSH服务器发起认证庆祝，SSH服务器对SSH客户端进行认证，认证方式有： 密码认证：客户端通过用户名密码的方式进行人恩正，将加密后的用户名密码发给服务器，服务器解密后与本地保存的用户名密码进行对比，并向客户端返回认证结果消息 SSH客户端向SSH服务器发送登录请求 SSH服务器将服务器公钥发送给SSH客户端 SSH客户端输入密码，使用服务器公钥加密后发送给SSH服务器 SSH服务器收到密文，使用服务器私钥解密得到密码，验证 如果有人在a步骤中截获SSH客户端发送的登录请求后，冒充SSH服务器将伪造的公钥发送给SSH客户端，就能够获得该用户的密码，所以，在首次登录SSH服务器时，SSH客户端上会提示公钥指纹，并询问用户是否确认登录，用户确认后公钥将被保存并信任，下次访问时，SSH客户端将会核对SSH服务器发来的公钥与本地保存的是否相同。这种方式适用于公布了公钥指纹的SSH服务器以及已登陆过正确SSH服务器的SSH客户端 密钥认证：客户端通过用户名、公钥及公钥算法等信息来与服务器认证 再进行SSH连接之前，SSH客户端需要先生成自己的公钥私钥对，并将自己的公钥存放在SSH服务器上 SSH客户端向SSH服务器发送登录请求 SSH服务器更具请求中的用户名等信息在本地搜索客户端的公钥，并用这个公钥加密一个随机数发送给客户端 SSH客户端使用自己的私钥对返回的信息进行解密，并发送给SSH服务器 SSH服务器验证SSH客户端解密的信息是否正确 密码-密钥认证：致用户需要同时满足密码和密钥认证才能登录 all认证：只要满足密码和密钥认证中的一种即可 会话请求认证通过后，SSH客户端向服务器发送会话请求，请求服务器提供耨中类型的服务 会话交互会话建立后，SSH服务器和客户端在该会话上进行数据信息的交互，双发发送的数据均使用会话密钥进行加解密 IDEA操作 配置Git，File - Settings - Version Control - Git - Path to Git executable：Git应用所在的路径 远程仓库创建 本地仓库初始化，VCS（Version Control） - Import into Version Control - Create Git Repository - 选择本地文件夹 提交，Git - √ - 勾选需要提交的文件 - Commit Message填写 - Commit 查看Log：左下角Version Control - Log 添加远程仓库：VCS - Git - Push - Define_remote改为远程仓库链接 克隆远程仓库：VCS - Checkout from Version Control - Git - 远程仓库链接 - Clone 创建分支：左下角Version Control - Log - 版本中右键new一个分支","categories":[{"name":"Dev Tools","slug":"Dev-Tools","permalink":"https://dangyoo.github.io/categories/Dev-Tools/"},{"name":"Git","slug":"Dev-Tools/Git","permalink":"https://dangyoo.github.io/categories/Dev-Tools/Git/"}],"tags":[{"name":"知识","slug":"知识","permalink":"https://dangyoo.github.io/tags/%E7%9F%A5%E8%AF%86/"}]},{"title":"Debezium","slug":"Debezium","date":"2023-08-27T09:49:03.000Z","updated":"2024-03-31T08:32:16.346Z","comments":true,"path":"2023/08/27/Debezium/","link":"","permalink":"https://dangyoo.github.io/2023/08/27/Debezium/","excerpt":"闲言碎语针对当前最常用的开源CDC工具Debezium，详细了解后再试用一下","text":"闲言碎语针对当前最常用的开源CDC工具Debezium，详细了解后再试用一下 架构Debezium是一组分布式服务，用于捕获数据库的更改，以便应用程序看到这些更改并作出响应。Debezium在一个变更事件流中记录每个数据库表中所有行级别的变更，应用程序可以读取这些流，来按顺序查看这些变更事件 Debezium构建在Apache Kafka之上，并提供KafkaConnect兼容连接器 架构有三种形式 架构一：基于Kafka Connector部署将Debezium当做Kafka的一个插件，将数据传输到Kafka中，再进行后续的处理，属于当前常用的架构 架构二：基于Debezium Server部署做数据传输用，传输到目标数据库中，支持的目标数据库较少，不常用 架构三：嵌入式引擎部署可以内置到其他应用中，例如Flink CDC 搭建根据所需要监控的数据库，下载对应的Connector，然后启动Zookeeper和Kafka 1234# 启动Zookeeper./zkServer.sh start# 启动Kafka./kafka-server-start.sh -daemon ../config/server.properties 监控MySQL首先要去开启MySQL的Binlog 12345678910111213141516171819202122sudo vim /etc/my.cnf# [mysqld]# server-id=1# log-bin=mysql-bin# binlog_format=row# 重启sudo systemctl restart mysql# 进入mysql后检查是否开启成功&gt; show variables like &#x27;%log_bin%&#x27;;+---------------------------------+-----------------------------+| Variable_name | Value |+---------------------------------+-----------------------------+| log_bin | ON || log_bin_basename | /var/lib/mysql/binlog || log_bin_index | /var/lib/mysql/binlog.index || log_bin_trust_function_creators | OFF || log_bin_use_v1_row_events | OFF || sql_log_bin | ON |+---------------------------------+-----------------------------+ 然后建立测试用的表 1234CREATE DATABASE testdb;USE testdb;CREATE TABLE stu(id INT, name TEXT, age INT);INSERT INTO stu VALUES(1, &#x27;zs&#x27;, 10); 安装debezium的MySQL Connector 1234cd /xx/debezium/connectorwget https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/1.7.1.Final/debezium-connector-mysql-1.7.1.Final-plugin.tar.gztar -zxvf debezium-connector-mysql-1.7.1.Final-plugin.tar.gz# 获得文件夹 /xx/debezium/connector/debezium-connector-mysql/ 将debezium-connector-mysql作为插件配置到Kafka中 123456789101112131415cd kafka/kafka_2.13-3.5.1/config/vim connect-distributed.properties # 连接器配置# plugin.path=/xx/debezium/connector# 修改配置后启动Kafka的连接器./connect-distributed.sh -daemon ../config/connect-distributed.properties# 日志为logs/connectDistributed.out# 查看Kafka服务状态curl -H &quot;Accept:application/json&quot; localhost:8083 # 8083为connector默认监听端口# &#123;&quot;version&quot;:&quot;3.5.1&quot;,&quot;commit&quot;:&quot;2c6fb6c54472e90a&quot;,&quot;kafka_cluster_id&quot;:&quot;Rh19dkgzT2KO9YIxZb6lOg&quot;&#125;# 查看连接器状态curl -H &quot;Accept:application/json&quot; localhost:8083/connectors/# []# 连接器为空，接下来需要注册一个连接器 注册连接器并查看数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314# 配置信息&#123; &quot;name&quot;: &quot;mysql-connector&quot;, # 连接器名称 &quot;config&quot;: &#123; &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;, # 官方内置类 &quot;database.hostname&quot;: &quot;localhost&quot;, &quot;database.port&quot;: &quot;3306&quot;, &quot;database.user&quot;: &quot;x&quot;, &quot;database.password&quot;: &quot;x&quot;, &quot;database.server.id&quot;: &quot;1&quot;, # my.cnf中配置的server-id &quot;database.server.name&quot;: &quot;cr7-demo&quot;, &quot;include.schema.changes&quot;: &quot;true&quot;, &quot;database.whitelist&quot;: &quot;testdb&quot;, &quot;table.whitlelist&quot;: &quot;stu&quot; &quot;database.history.kafka.boostrap.servers&quot;: &quot;localhost:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;cr7-schema-changes-inventory&quot; # 存储DDL表结构变化数据 &#125;&#125;# 把以上配置信息写到了一个json文件中：my-connector.json# 使用curl注册curl -i -X POST -H &quot;Content-Type:application/json&quot; --data @my-connector.json localhost:8083/connectors# HTTP/1.1 201 Created# 检查连接器curl -H &quot;Accept:application/json&quot; localhost:8083/connectors/# [&quot;mysql-connector&quot;]# 检查连接器运行状态curl localhost:8083/connectors/mysql-connector/status -s | jq# &#123;# &quot;name&quot;: &quot;mysql-connector&quot;,# &quot;config&quot;: &#123;# &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,# &quot;database.user&quot;: &quot;x&quot;,# &quot;database.server.id&quot;: &quot;1&quot;,# &quot;database.history.kafka.bootstrap.servers&quot;: &quot;xx.xx.xx.xx:9092&quot;,# &quot;database.history.kafka.topic&quot;: &quot;cr7-schema-changes-inventory&quot;,# &quot;database.server.name&quot;: &quot;cr7-demo&quot;,# &quot;database.port&quot;: &quot;3306&quot;,# &quot;include.schema.changes&quot;: &quot;true&quot;,# &quot;database.hostname&quot;: &quot;localhost&quot;,# &quot;database.password&quot;: &quot;x&quot;,# &quot;table.whitlelist&quot;: &quot;stu&quot;,# &quot;name&quot;: &quot;mysql-connector&quot;,# &quot;database.whitelist&quot;: &quot;testdb&quot;# &#125;,# &quot;tasks&quot;: [# &#123;# &quot;connector&quot;: &quot;mysql-connector&quot;,# &quot;task&quot;: 0# &#125;# ],# &quot;type&quot;: &quot;source&quot;# &#125;# 用最新2.3.2Final的时候这一步报错 Error configuring an instance of KafkaSchemaHistory# 没找到解决办法，后来改成了1.7.1Final版本# 删除连接器curl -X DELETE localhost:8083/connectors/test-mysql-connector# 查看Kafka中的Topic./kafka-topics.sh --list --bootstrap-server localhost:9092# cr7-demo# cr7-demo.testdb.stu# cr7-schema-changes-inventory# 查看Topic中的数据./kafka-console-consumer.sh \\--bootstrap-server localhost:9092 \\--topic cr7-demo.testdb.stu \\--from-beginning# &#123;# &quot;before&quot;:null,# &quot;after&quot;:&#123;# &quot;id&quot;:1,# &quot;name&quot;:&quot;zs&quot;,# &quot;age&quot;:10# &#125;,# &quot;source&quot;:&#123;# &quot;version&quot;:&quot;1.7.1.Final&quot;,# &quot;connector&quot;:&quot;mysql&quot;,# &quot;name&quot;:&quot;cr7-demo&quot;,# &quot;ts_ms&quot;:1693128849580,# &quot;snapshot&quot;:&quot;last&quot;,# &quot;db&quot;:&quot;testdb&quot;,# &quot;sequence&quot;:null,# &quot;table&quot;:&quot;stu&quot;,# &quot;server_id&quot;:0,# &quot;gtid&quot;:null,# &quot;file&quot;:&quot;binlog.000005&quot;,# &quot;pos&quot;:1901,# &quot;row&quot;:0,# &quot;thread&quot;:null,# &quot;query&quot;:null# &#125;,# &quot;op&quot;:&quot;r&quot;,# &quot;ts_ms&quot;:1693128849582,# &quot;transaction&quot;:null# &#125;# 查看Topic中的数据./kafka-console-consumer.sh \\--bootstrap-server localhost:9092 \\--topic cr7-demo \\--from-beginning# &#123;# &quot;source&quot;:&#123;# &quot;version&quot;:&quot;1.7.1.Final&quot;,# &quot;connector&quot;:&quot;mysql&quot;,# &quot;name&quot;:&quot;cr7-demo&quot;,# &quot;ts_ms&quot;:1693128849464,# &quot;snapshot&quot;:&quot;true&quot;,# &quot;db&quot;:&quot;testdb&quot;,# &quot;sequence&quot;:null,# &quot;table&quot;:&quot;stu&quot;,# &quot;server_id&quot;:0,# &quot;gtid&quot;:null,# &quot;file&quot;:&quot;binlog.000005&quot;,# &quot;pos&quot;:1901,# &quot;row&quot;:0,# &quot;thread&quot;:null,# &quot;query&quot;:null# &#125;,# &quot;databaseName&quot;:&quot;testdb&quot;,# &quot;schemaName&quot;:null,# &quot;ddl&quot;:&quot;CREATE TABLE `stu` (\\n `id` int DEFAULT NULL,\\n `name` text,\\n `age` int DEFAULT NULL\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci&quot;,# &quot;tableChanges&quot;:[# &#123;# &quot;type&quot;:&quot;CREATE&quot;,# &quot;id&quot;:&quot;\\&quot;testdb\\&quot;.\\&quot;stu\\&quot;&quot;,# &quot;table&quot;:&#123;# &quot;defaultCharsetName&quot;:&quot;utf8mb4&quot;,# &quot;primaryKeyColumnNames&quot;:[# ],# &quot;columns&quot;:[# &#123;# &quot;name&quot;:&quot;id&quot;,# &quot;jdbcType&quot;:4,# &quot;nativeType&quot;:null,# &quot;typeName&quot;:&quot;INT&quot;,# &quot;typeExpression&quot;:&quot;INT&quot;,# &quot;charsetName&quot;:null,# &quot;length&quot;:null,# &quot;scale&quot;:null,# &quot;position&quot;:1,# &quot;optional&quot;:true,# &quot;autoIncremented&quot;:false,# &quot;generated&quot;:false# &#125;,# &#123;# &quot;name&quot;:&quot;name&quot;,# &quot;jdbcType&quot;:12,# &quot;nativeType&quot;:null,# &quot;typeName&quot;:&quot;TEXT&quot;,# &quot;typeExpression&quot;:&quot;TEXT&quot;,# &quot;charsetName&quot;:&quot;utf8mb4&quot;,# &quot;length&quot;:null,# &quot;scale&quot;:null,# &quot;position&quot;:2,# &quot;optional&quot;:true,# &quot;autoIncremented&quot;:false,# &quot;generated&quot;:false# &#125;,# &#123;# &quot;name&quot;:&quot;age&quot;,# &quot;jdbcType&quot;:4,# &quot;nativeType&quot;:null,# &quot;typeName&quot;:&quot;INT&quot;,# &quot;typeExpression&quot;:&quot;INT&quot;,# &quot;charsetName&quot;:null,# &quot;length&quot;:null,# &quot;scale&quot;:null,# &quot;position&quot;:3,# &quot;optional&quot;:true,# &quot;autoIncremented&quot;:false,# &quot;generated&quot;:false# &#125;# ]# &#125;# &#125;# ]# &#125;# 查看Topic中的数据./kafka-console-consumer.sh \\--bootstrap-server localhost:9092 \\--topic cr7-schema-changes-inventory \\--from-beginning# &#123;# &quot;source&quot; : &#123;# &quot;server&quot; : &quot;cr7-demo&quot;# &#125;,# &quot;position&quot; : &#123;# &quot;ts_sec&quot; : 1693128848,# &quot;file&quot; : &quot;binlog.000005&quot;,# &quot;pos&quot; : 1901,# &quot;snapshot&quot; : true# &#125;,# &quot;databaseName&quot; : &quot;&quot;,# &quot;ddl&quot; : &quot;SET character_set_server=utf8mb4, collation_server=utf8mb4_0900_ai_ci&quot;,# &quot;tableChanges&quot; : [ ]# &#125;# &#123;# &quot;source&quot; : &#123;# &quot;server&quot; : &quot;cr7-demo&quot;# &#125;,# &quot;position&quot; : &#123;# &quot;ts_sec&quot; : 1693128849,# &quot;file&quot; : &quot;binlog.000005&quot;,# &quot;pos&quot; : 1901,# &quot;snapshot&quot; : true# &#125;,# &quot;databaseName&quot; : &quot;testdb&quot;,# &quot;ddl&quot; : &quot;DROP TABLE IF EXISTS `testdb`.`stu`&quot;,# &quot;tableChanges&quot; : [ ]# &#125;# &#123;# &quot;source&quot; : &#123;# &quot;server&quot; : &quot;cr7-demo&quot;# &#125;,# &quot;position&quot; : &#123;# &quot;ts_sec&quot; : 1693128849,# &quot;file&quot; : &quot;binlog.000005&quot;,# &quot;pos&quot; : 1901,# &quot;snapshot&quot; : true# &#125;,# &quot;databaseName&quot; : &quot;testdb&quot;,# &quot;ddl&quot; : &quot;DROP DATABASE IF EXISTS `testdb`&quot;,# &quot;tableChanges&quot; : [ ]# &#125;# &#123;# &quot;source&quot; : &#123;# &quot;server&quot; : &quot;cr7-demo&quot;# &#125;,# &quot;position&quot; : &#123;# &quot;ts_sec&quot; : 1693128849,# &quot;file&quot; : &quot;binlog.000005&quot;,# &quot;pos&quot; : 1901,# &quot;snapshot&quot; : true# &#125;,# &quot;databaseName&quot; : &quot;testdb&quot;,# &quot;ddl&quot; : &quot;CREATE DATABASE `testdb` CHARSET utf8mb4 COLLATE utf8mb4_0900_ai_ci&quot;,# &quot;tableChanges&quot; : [ ]# &#125;# &#123;# &quot;source&quot; : &#123;# &quot;server&quot; : &quot;cr7-demo&quot;# &#125;,# &quot;position&quot; : &#123;# &quot;ts_sec&quot; : 1693128849,# &quot;file&quot; : &quot;binlog.000005&quot;,# &quot;pos&quot; : 1901,# &quot;snapshot&quot; : true# &#125;,# &quot;databaseName&quot; : &quot;testdb&quot;,# &quot;ddl&quot; : &quot;USE `testdb`&quot;,# &quot;tableChanges&quot; : [ ]# &#125;# &#123;# &quot;source&quot; : &#123;# &quot;server&quot; : &quot;cr7-demo&quot;# &#125;,# &quot;position&quot; : &#123;# &quot;ts_sec&quot; : 1693128849,# &quot;file&quot; : &quot;binlog.000005&quot;,# &quot;pos&quot; : 1901,# &quot;snapshot&quot; : true# &#125;,# &quot;databaseName&quot; : &quot;testdb&quot;,# &quot;ddl&quot; : &quot;CREATE TABLE `stu` (\\n `id` int DEFAULT NULL,\\n `name` text,\\n `age` int DEFAULT NULL\\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci&quot;,# &quot;tableChanges&quot; : [ &#123;# &quot;type&quot; : &quot;CREATE&quot;,# &quot;id&quot; : &quot;\\&quot;testdb\\&quot;.\\&quot;stu\\&quot;&quot;,# &quot;table&quot; : &#123;# &quot;defaultCharsetName&quot; : &quot;utf8mb4&quot;,# &quot;primaryKeyColumnNames&quot; : [ ],# &quot;columns&quot; : [ &#123;# &quot;name&quot; : &quot;id&quot;,# &quot;jdbcType&quot; : 4,# &quot;typeName&quot; : &quot;INT&quot;,# &quot;typeExpression&quot; : &quot;INT&quot;,# &quot;charsetName&quot; : null,# &quot;position&quot; : 1,# &quot;optional&quot; : true,# &quot;autoIncremented&quot; : false,# &quot;generated&quot; : false# &#125;, &#123;# &quot;name&quot; : &quot;name&quot;,# &quot;jdbcType&quot; : 12,# &quot;typeName&quot; : &quot;TEXT&quot;,# &quot;typeExpression&quot; : &quot;TEXT&quot;,# &quot;charsetName&quot; : &quot;utf8mb4&quot;,# &quot;position&quot; : 2,# &quot;optional&quot; : true,# &quot;autoIncremented&quot; : false,# &quot;generated&quot; : false# &#125;, &#123;# &quot;name&quot; : &quot;age&quot;,# &quot;jdbcType&quot; : 4,# &quot;typeName&quot; : &quot;INT&quot;,# &quot;typeExpression&quot; : &quot;INT&quot;,# &quot;charsetName&quot; : null,# &quot;position&quot; : 3,# &quot;optional&quot; : true,# &quot;autoIncremented&quot; : false,# &quot;generated&quot; : false# &#125; ]# &#125;# &#125; ]# &#125; 监控PostgreSQL首先配置PostgreSQL 123456789101112131415161718psql -c &quot;show config_file&quot;# 查找配置文件路径# /etc/postgresql/13/main/postgresql.confsudo vim /etc/postgresql/13/main/postgresql.conf# 修改listen_address = &#x27;*&#x27;# 新增# wal_level = logical# max_wal_senders = 2# max_replication_slots = 1# 配置pg_hba.confsudo vim /etc/postgresql/13/main/pg_hba.conf# 新增# host all all 0.0.0.0/0 scram-sha-256# host replication all 0.0.0.0.0 trust# 重启数据库sudo systemctl restart postgresql 然后建立测试用表 12345! psqlCREATE DATABASE testdb OWNER x;! psql -d testdb;CREATE TABLE stu(id INT, name VARCHAR, age INT);INSERT INTO stu VALUES(1, &#x27;zs&#x27;, 10); 安装PostgreSQL Connector 123wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.7.1.Final/debezium-connector-postgres-1.7.1.Final-plugin.tar.gztar -zxvf debezium-connector-postgres-1.7.1.Final-plugin.tar.gzcd debezium-connector-postgres/ 注册连接器并查看数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# 配置信息&#123; &quot;name&quot;: &quot;postgresql-connector&quot;, # 连接器名称 &quot;config&quot;: &#123; &quot;connector.class&quot;: &quot;io.debezium.connector.postgresql.PostgresConnector&quot;, # 官方内置类 &quot;database.hostname&quot;: &quot;localhost&quot;, &quot;database.port&quot;: &quot;5432&quot;, &quot;database.user&quot;: &quot;x&quot;, &quot;database.password&quot;: &quot;x&quot;, &quot;database.dbname&quot;: &quot;testdb&quot;, &quot;database.server.name&quot;: &quot;pgsqldemo&quot;, &quot;plugin.name&quot;: &quot;pgoutput&quot; &#125;&#125;# 写入postgresql-connector.json文件中# 重启Kafka连接器，这里不重启的话会报500 Failed to find any class that implements Connector错误# 使用curl注册curl -i -X POST -H &quot;Content-Type:application/json&quot; --data @postgresql-connector.json localhost:8083/connectors# HTTP/1.1 201 Created# 检查连接器curl -H &quot;Accept:application/json&quot; localhost:8083/connectors/# [&quot;postgresql-connector&quot;,&quot;mysql-connector&quot;]# 检查连接器运行状态curl localhost:8083/connectors/postgresql-connector/status -s | jq# &#123;# &quot;name&quot;: &quot;postgresql-connector&quot;,# &quot;connector&quot;: &#123;# &quot;state&quot;: &quot;RUNNING&quot;,# &quot;worker_id&quot;: &quot;127.0.1.1:8083&quot;# &#125;,# &quot;tasks&quot;: [# &#123;# &quot;id&quot;: 0,# &quot;state&quot;: &quot;RUNNING&quot;,# &quot;worker_id&quot;: &quot;127.0.1.1:8083&quot;# &#125;# ],# &quot;type&quot;: &quot;source&quot;# &#125;# 查看Kafka中的Topic./kafka-topics.sh --list --bootstrap-server localhost:9092# pgsqldemo.public.stu# 查看Topic中的数据./kafka-console-consumer.sh \\--bootstrap-server localhost:9092 \\--topic pgsqldemo.public.stu \\--from-beginning# &#123;# &quot;before&quot;:null,# &quot;after&quot;:&#123;# &quot;id&quot;:1,# &quot;name&quot;:&quot;zs&quot;,# &quot;age&quot;:10# &#125;,# &quot;source&quot;:&#123;# &quot;version&quot;:&quot;1.7.1.Final&quot;,# &quot;connector&quot;:&quot;postgresql&quot;,# &quot;name&quot;:&quot;pgsqldemo&quot;,# &quot;ts_ms&quot;:1693214757722,# &quot;snapshot&quot;:&quot;last&quot;,# &quot;db&quot;:&quot;testdb&quot;,# &quot;sequence&quot;:&quot;[null,\\&quot;23256472\\&quot;]&quot;,# &quot;schema&quot;:&quot;public&quot;,# &quot;table&quot;:&quot;stu&quot;,# &quot;txId&quot;:516,# &quot;lsn&quot;:23256472,# &quot;xmin&quot;:null# &#125;,# &quot;op&quot;:&quot;r&quot;,# &quot;ts_ms&quot;:1693214757727,# &quot;transaction&quot;:null# &#125; 常见问题snapshot.mode &#x3D; initial，控制Debezium在首次监控某个表的时候会先同步所有的历史数据，如果在同步历史数据的过程中不能避免数据更新，则使用snapshot.locking.mode &#x3D; minimal来给表加锁 默认一个表的监控数据会写入一个Topic中，支持通过topic routing设置同样表结构的表可以写入同一个Topic中","categories":[{"name":"Data","slug":"Data","permalink":"https://dangyoo.github.io/categories/Data/"},{"name":"Integration","slug":"Data/Integration","permalink":"https://dangyoo.github.io/categories/Data/Integration/"},{"name":"Debezium","slug":"Data/Integration/Debezium","permalink":"https://dangyoo.github.io/categories/Data/Integration/Debezium/"}],"tags":[{"name":"知识","slug":"知识","permalink":"https://dangyoo.github.io/tags/%E7%9F%A5%E8%AF%86/"}]},{"title":"苹果日历订阅源开发","slug":"苹果日历订阅源开发","date":"2023-08-26T08:47:10.000Z","updated":"2024-03-31T08:40:03.215Z","comments":true,"path":"2023/08/26/苹果日历订阅源开发/","link":"","permalink":"https://dangyoo.github.io/2023/08/26/%E8%8B%B9%E6%9E%9C%E6%97%A5%E5%8E%86%E8%AE%A2%E9%98%85%E6%BA%90%E5%BC%80%E5%8F%91/","excerpt":"闲言碎语近期为了学英语，用美区账号下了些App，结果把手机地区、语言啥的改了之后，发现之前的日历节日都不见了，看到有Subscribe的选项，就想着自己做一个算了。","text":"闲言碎语近期为了学英语，用美区账号下了些App，结果把手机地区、语言啥的改了之后，发现之前的日历节日都不见了，看到有Subscribe的选项，就想着自己做一个算了。 查怎么实现这东西网上实现的已经很多了，随手打开一个Star最多的china-holidy-calender，看到订阅地址是一个.ics的文件，打开后查看文件格式 里边的字段对应到iOS日历里的信息，Git上给的是20210501的图 那就比较简单了，首先肯定是把需要从网上某个地方获取的字段提取出来，然后把这些数据从某个地方拿下来（存到数据库里），再生成一个对应格式的ics文件 提取字段DTSTART：日程开始时间 DTEND：日程结束时间 每一天就用日历里的All-day类型日程即可，DTSTART &#x3D; DTEND SUMMARY：日程标题 DESCRIPTION：日程备注 备注不需要搞太复杂，把节日信息放上就够了，甚至只要标题有就够了，不需要备注 没了 获取数据看Git上这个项目是从政府网上拿的数据，搜了一下只有2022年和2023年发布过，看起来如果有变更还要再做维护的样子 找了一下Python中有一个包chinesecalendar 先基于这个数据做一个实现吧 chinesecalendar123456789101112# !pip install chinesecalendar 先安装# 试用import datetimeimport chinese_calendar as calendarcalendar.get_holiday_detail(datetime.date(2023,9,29))# (True, &#x27;Mid-autumn Festival&#x27;)calendar.get_holiday_detail(datetime.date(2023,9,28))# (False, None)calendar.get_holiday_detail(datetime.date(2023,10,7))# (False, &#x27;National Day&#x27;)calendar.get_holiday_detail(datetime.date(2023,10,14))# (True, None) 主要用get_holiday_detail就够了，返回一个Tuple类型，两个元素，第一个元素代表是否节假日，第二个元素代表哪个节假日 此外如果调休上班，周六周日会标记为（False，调休的节日名） 可以加上一个周六周日的判断，结合这个结果来判断是否为调休上班 日历中标记所有节假日以及调休上班的日期 脚本第一步先把要的数据拿到并做好结构化 第二步把数据拼成想要的ics文件格式 第一步：拿数据写一个脚本获取数据，只保存需要标记的日期信息，存到PostgreSQL中 想把假期和调休分别标记，所以多加一个标识，表结构定义为 12345678910111213# CREATE TABLE calendar_data( the_date TEXT, --COMMENT &#x27;日期&#x27;, the_type TEXT, --COMMENT &#x27;标记类型，1:假期|2:调休&#x27;, mark TEXT --COMMENT &#x27;节日信息&#x27; );# \\d calendar_data; Table &quot;public.calendar_data&quot; Column | Type | Collation | Nullable | Default ----------+------+-----------+----------+--------- the_date | text | | | the_type | text | | | mark | text | | | 12345678910111213141516171819202122232425262728293031323334# get_calendar_data.pyimport datetimeimport chinese_calendar as calendarimport psycopg2# 主要用到的函数：# 当天时间# today = datetime.date.today() # datetime.date(2023, 8, 26)# 判断周几，返回1~7# today.isoweekday() # 6# 下一天# next_day = today + datetime.timedelta(days=1) # datetime.date(2023, 8, 27)# 节假日信息# calendar.get_holiday_detail(datetime.date(2023,10,7))# (False, &#x27;National Day&#x27;)# 连接数据库conn = psycopg2.connect(database=&#x27;x&#x27;,user=&#x27;x&#x27;,password=&#x27;x&#x27;,host=&#x27;127.0.0.1&#x27;,port=&#x27;5432&#x27;)cursor = conn.cursor()# 循环日期，查找返回结果# 如果结果需要插入表中，则执行写入today = datetime.date.today()# 这个库只提供当年的数据left_days = (datetime.date(today.year+1,1,1) - today).daysfor i in range(left_days): the_date = today+datetime.timedelta(days=i) calendar_result = calendar.get_holiday_detail(the_date) if calendar_result[1]: mark = calendar_result[1] the_type = 1 if calendar_result[0] else 2 cursor.execute(&quot;&quot;&quot;INSERT INTO calendar_data(the_date, the_type, mark) VALUES(&#x27;%s&#x27;, &#x27;%s&#x27;, &#x27;%s&#x27;)&quot;&quot;&quot; % (the_date.strftime(&#x27;%Y%m%d&#x27;), the_type, mark))# 循环结束后一次性commitconn.commit() 执行后去数据库里查看 1234567891011121314# select * from calendar_data; the_date | the_type | mark ----------+----------+--------------------- 20230929 | 1 | Mid-autumn Festival 20230930 | 1 | National Day 20231001 | 1 | National Day 20231002 | 1 | National Day 20231003 | 1 | National Day 20231004 | 1 | National Day 20231005 | 1 | National Day 20231006 | 1 | National Day 20231007 | 2 | National Day 20231008 | 2 | National Day(10 rows) 第二步：写文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 将文件格式分为Event和其他两部分def return_ics_header(the_type): if the_type == 1: calname = &#x27;节假日&#x27; else: calname = &#x27;调休&#x27; return &quot;BEGIN:VCALENDAR\\n&quot; \\ + &quot;PRODID:NULL\\n&quot; \\ + &quot;VERSION:2.0\\n&quot; \\ + &quot;CALSCALE:GREGORIAN\\n&quot; \\ + &quot;METHOD:PUBLISH\\n&quot; \\ + f&quot;X-WR-CALNAME:&#123;calname&#125;\\n&quot; \\ + &quot;X-WR-TIMEZONE:Asia/Shanghai\\n&quot; \\ + f&quot;X-WR-CALDESC:&#123;calname&#125;日历\\n&quot; \\ + &quot;BEGIN:VTIMEZONE\\n&quot; \\ + &quot;TZID:Asia/Shanghai\\n&quot; \\ + &quot;X-LIC-LOCATION:Asia/Shanghai\\n&quot; \\ + &quot;BEGIN:STANDARD\\n&quot; \\ + &quot;TZOFFSETFROM:+0800\\n&quot; \\ + &quot;TZOFFSETTO:+0800\\n&quot; \\ + &quot;TZNAME:CST\\n&quot; \\ + &quot;DTSTART:19700101T000000\\n&quot; \\ + &quot;END:STANDARD\\n&quot; \\ + &quot;END:VTIMEZONE\\n&quot; def return_ics_event(the_date, mark, the_type): if the_type == 1: caldesc = &#x27;Holiday &#x27; else: caldesc = &#x27;Work &#x27; return &quot;BEGIN:VEVENT\\n&quot; \\ + f&quot;DTSTART;VALUE=DATE:&#123;the_date&#125;\\n&quot; \\ + f&quot;DTEND;VALUE=DATE:&#123;the_date&#125;\\n&quot; \\ + f&quot;DTSTAMP:&#123;the_date&#125;T000001\\n&quot; \\ + f&quot;UID:&#123;the_date&#125;T000001_dy\\n&quot; \\ + f&quot;CREATED:&#123;the_date&#125;T000001\\n&quot; \\ + f&quot;DESCRIPTION:&#123;mark&#125;\\n&quot; \\ + f&quot;LAST-MODIFIED:20230826T00:00:01\\n&quot; \\ + &quot;SEQUENCE:0\\n&quot; \\ + &quot;STATUS:CONFIRMED\\n&quot; \\ + f&quot;SUMMARY:&#123;caldesc&#125;&#123;mark&#125;\\n&quot; \\ + &quot;TRANSP:TRANSPARENT\\n&quot; \\ + &quot;END:VEVENT\\n&quot;# 这里三引号内容输出的时候每一行前边会有空格，用inspect.cleandoc做处理# 结果最后订阅后显示validation failed，不知道是哪里格式有问题，把这个换一下# 试了下其他人gitee上的ics也显示同样的问题，换github试一下# 整了半天，是头文件中VERSION必须是2.0# 根据类型生成两个ics文件 def mk_ics_file(the_type): pwd = &quot;/home/dy/gzh/my_calendar/&quot; fname = pwd+&quot;calendar%s.ics&quot;%the_type header = return_ics_header(the_type) ics_event = &#x27;&#x27; conn = psycopg2.connect(database=&#x27;x&#x27;,user=&#x27;x&#x27;,password=&#x27;x&#x27;,host=&#x27;127.0.0.1&#x27;,port=&#x27;5432&#x27;) cursor = conn.cursor() cursor.execute(&quot;SELECT the_date, mark FROM calendar_data WHERE the_type = &#x27;%s&#x27;&quot;%the_type) result = cursor.fetchall() for each_result in result: ics_event = ics_event + return_ics_event(each_result[0], each_result[1], the_type) # file_content = inspect.cleandoc(header) + &#x27;\\n&#x27; + inspect.cleandoc(ics_event) + &quot;\\nEND:VCALENDAR&quot; file_content = header + ics_event + &quot;END:VCALENDAR&quot; with open(fname, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as f: f.write(file_content) 生成两个文件calendar1.ics和calendar2.ics 分别对应节假日日历和调休日历 第三步：上传文件用Git来保存文件，然后订阅 坑了一个小时的一个点：头文件中VERSION必须是2.0 订阅地址：(服务器没再续了，就这吧) 12假期：https://raw.githubusercontent.com/Dangyoo/my_calendar/master/calendar1.ics调休：https://raw.githubusercontent.com/Dangyoo/my_calendar/master/calendar2.ics","categories":[{"name":"小项目","slug":"小项目","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/"},{"name":"日历订阅","slug":"小项目/日历订阅","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E6%97%A5%E5%8E%86%E8%AE%A2%E9%98%85/"}],"tags":[{"name":"实践","slug":"实践","permalink":"https://dangyoo.github.io/tags/%E5%AE%9E%E8%B7%B5/"}]},{"title":"Zookeeper","slug":"Zookeeper","date":"2023-08-24T10:06:03.000Z","updated":"2024-03-31T08:32:16.342Z","comments":true,"path":"2023/08/24/Zookeeper/","link":"","permalink":"https://dangyoo.github.io/2023/08/24/Zookeeper/","excerpt":"闲言碎语对动物园管理员的了解一直停留在“这是一个管理Hadoop各种服务的服务“，但具体是怎么实现管理协调功能的，不太清楚，遂搞清楚一下","text":"闲言碎语对动物园管理员的了解一直停留在“这是一个管理Hadoop各种服务的服务“，但具体是怎么实现管理协调功能的，不太清楚，遂搞清楚一下 Apache ZookeeperZookeeper是一种分布式协调服务，用于管理大型主机。在分布式环境中协调和管理服务是一个复杂的过程，Zookeeper通过其简单的架构和API解决了这个问题，允许开发人员专注于核心应用程序逻辑，而不用担心应用程序的分布式特性 应用场景 分布式协调组件：一旦某个节点发生改变，就会通知所有监听方改变自己的值 分布式锁：依靠ZAB协议实现锁的强一致性 无状态化实现：作为数据中心维护各种状态数据 搭建1234567891011121314151617181920212223242526272829303132# 下载Zookeeper安装包wget https://dlcdn.apache.org/zookeeper/zookeeper-3.9.0/apache-zookeeper-3.9.0-bin.tar.gz# 解压缩tar -zxvf apache-zookeeper-3.9.0.tar.gz# 运行服务cd apache-zookeeper-3.9.0-bin/bin./zkServer.sh# 提示Error: JAVA_HOME is not set and java could not be found in PATH.# 没有安装Java，Zookeeper是Java实现，所以要先安装Javasudo apt install default-jdk# 安装默认JRE的话会安装JAVA 11./zkServer.sh# 提示../conf/zoo.cfg: No such file or directorycd ../conf/cp zoo_sample.cfg zoo.cfg # 使用示例配置cd ../bin/./zkServer.sh start# 检查是否成功启动ps -ef|grep zookeeper# 错误提示# 提示Starting zookeeper ... FAILED TO START./zkServer.sh start-foreground# 查看启动失败原因# Error: Could not find or load main class org.apache.zookeeper.server.quorum.QuorumPeerMain# Caused by: java.lang.ClassNotFoundException: org.apache.zookeeper.server.quorum.QuorumPeerMain# 下载错了包，要下载的是带有‘-bin’的包wget https://www.apache.org/dyn/closer.lua/zookeeper/zookeeper-3.9.0/apache-zookeeper-3.9.0-bin.tar.gztar -zxvf apache-zookeeper-3.9.0-bin.tar.gz# 提示gzip: stdin: not in gzip format# 后来发现是链接问题，真正的下载链接要点进去# 参考文章https://www.amwalle.com/more/working/20210516-gzip-stdin-not-in-gzip-format.html 常见命令1234567891011121314151617181920./zkServer.sh status# 查看服务状态# status提示Error contacting service. It is probably not running.# 说明还是没有启动成功# ./zkServer.sh start-foreground查看原因# Problem starting AdminServer on address 0.0.0.0, port 8080 and command URL /commands# 看起来是有个Admin服务想绑定到8080端口但是被占用了# vim ../conf/zoo.cfg# 编辑配置文件，加上一行# admin.serverPort=8081# ./zkServer.sh start# ./zkServer.sh status# 提示Mode: standalone，启动成功./zkServer.sh stop# 停止服务状态./zkCli.sh# 进入zookeeper控制台 数据模型Zookeeper内部保存数据的模型为树模型，数据存储基于节点，节点称为Znode，Znode的引用方式是路径引用，路径类似于文件路径 12345678910111213141516171819202122232425262728293031323334353637383940ls /# [zookeeper]# 查看根节点create /test1ls /# [test1, zookeeper]# 创建一个新节点create /test2ls /# [test2, test1, zookeeper]create /test2/sub2 abcget /test2/sub2# abccreate /test2/sub1ls /test2# [sub2, sub1]set /test2/sub1 bbcget /test2/sub1# bbc# 存入数据并获取ls -R /# /# /test1# /test2# /zookeeper# /test2/sub1# /test2/sub2# /zookeeper/config# /zookeeper/quota# 递归查询所有子节点delete /test1 # 删除节点，有根节点时无法删除deleteall /test2 # 删除根节点及其子节点delete -v 0 /test3 # 乐观锁删除# 如果这个节点被修改，则不能删除，如果没有，则能够删除# 并行时Znode可能被其他机器节点修改 Znode内容每个Znode中，包含了四个部分： data：节点中的数据 acl：权限 c：create创建权限，允许在该节点下创建子节点 w：write更新权限，允许更新该节点的数据 r：read读取权限，允许读取该节点的内容以及子节点的列表信息 d：delete删除权限，允许删除该节点的子节点 a：admin管理权限，允许对该节点进行acl权限设置 stat：描述当前节点的元数据 child：当前节点的子节点 123456789101112131415# 注册当前会话的账号和密码addauth digest username:pwd# 创建节点并设置权限create /test-node abcd auth:username:pwd:cdwra# 在另一个会话中，必须先使用账号密码，才能拥有操作该节点的权限get /test-node# Insufficient permission : /test-node# 另一个会话中访问不到addauth digest username:pwdget /test-node# abcd# 给这个会话添加权限后再进行访问# 创建节点并设置权限后提示KeeperErrorCode = InvalidACL for /test-node# 少写了关键词auth: 123456789101112131415161718192021222324252627282930313233343536373839[zk: localhost:2181(CONNECTED) 15] get -s /test2nullcZxid = 0x4 # 创建节点的事务IDctime = Wed Aug 23 19:23:09 CST 2023 # 创建节点的时间mZxid = 0x4 # 修改节点的事务IDmtime = Wed Aug 23 19:23:09 CST 2023 # 修改节点的时间pZxid = 0x7 # 添加和删除子节点的事务IDcversion = 2 #dataVersion = 0 # 节点内的数据版本，每更新一次版本+1aclVersion = 0 # 节点内的权限版本ephemeralOwner = 0x0 # 如果当前节点是临时节点，该值是当前节点所有者的session id，否则为0dataLength = 0 # 节点数据的长度numChildren = 2 # 子节点个数[zk: localhost:2181(CONNECTED) 17] get -s /test2/sub1abccZxid = 0x7ctime = Wed Aug 23 19:23:19 CST 2023mZxid = 0x7mtime = Wed Aug 23 19:23:19 CST 2023pZxid = 0x7cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 0[zk: localhost:2181(CONNECTED) 16] get -s /test2/sub2bbccZxid = 0x5ctime = Wed Aug 23 19:23:11 CST 2023mZxid = 0x8mtime = Wed Aug 23 19:23:41 CST 2023pZxid = 0x5cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 0 类型持久节点：创建出的节点在会话结束后依然存在，create path 持久序号（sequence）节点：创建出的节点，根据先后顺序，会在节点之后带上一个数值，越后执行数值越大，适用于分布式锁的应用场景，create -s path 临时（ephemeral）节点：会话结束后自动被删除的节点，通过这个特性可以实现服务注册与发现的效果，create -e path 临时序号节点：适用于临时的分布式锁，create -s -e path 容器（Container）节点：当容器中没有任何子节点，该容器节点会被zk定期删除，create -c path TTL（Time-To-Live）节点：可以指定节点的到期时间，到期后被zk定期删除，只能通过系统配置zookeeper.extendedTypesEnabled&#x3D;true开启，create -t ttl path 持久化机制在恢复时先恢复快照文件中的数据到内存中，再用日志文件中的数据做增量恢复，以此来提高回复速度 事务日志log，zk把执行的命令以日志形式保存在dataLogDir指定的路径中，如果没有指定dataLogDir，则保存到dataDir路径中 数据快照snapshot，zk会在一定的时间间隔内做一次内存数据的快照，把该时刻的内存数据保存在快照文件中 分布式锁分布式锁其实就是，控制分布式系统不同进程共同访问共享资源的一种锁的实现 如果不同的系统或同一个系统的不同主机之间共享了某个资源，往往需要互斥来防止彼此干扰，以保证一致性 锁的种类 读锁：大家都可以读，要想上读锁的前提是之前没有写锁 写锁：只有得到写锁才能写，要想上写锁的前提是之前没有任何锁 数据正在进行写入的时候，是不能读取的；数据想要写入，不能正在被读取或写入 读锁zk如何上读锁 创建一个临时序号节点，节点的数据是read，表示是读锁 获取当前zk中序号比自己小的所有节点 判断最小节点是否是读锁 如果不是读锁，则上锁失败，为最小节点设置监听，阻塞等待，zk的watch机制会在最小节点发生变化时通知当前节点，于是再执行第二步的流程 如果是读锁，则上锁成功 写锁zk如何上写锁 创建一个临时序号节点，节点的数据是write，表示是写锁 获取zk中所有的子节点 判断自己是否是最小的节点 如果是，则上写锁成功 如果不是，说明前面还有锁，则上锁失败，监听最小节点，如果最小节点有变化，则回到第二步 羊群效应如果用上述的上锁方式，只要节点发生变化，就会出发其他节点的监听事件，这样的话对zk的压力非常大，调整成链式监听，可以解决该问题，即只监听自己序号的上一个节点，而不是监听最小节点 Watch机制可以将Watch理解为注册在特定Znode上的触发器，当这个Znode发生改变，即调用了create、delete、setData方法时，将会出发Znode上注册的对应事件，请求Watch的客户端会接收到异步通知 具体操作get -w &#x2F;test客户端对&#x2F;test这个Znode进行监听 当另一个客户端对被监听节点进行改变时，当前客户端会受到WatchedEvent 交互过程客户端调用getData方法，参数为true 服务端收到请求，返回节点数据，并且在对应的哈希表里插入被Watch的Znode路径，以及Watcher列表 当被Watch的Znode删除时，服务端会查找哈希表，找到对应Znode的所有Watcher，异步通知客户端，并且删除哈希表中对应的Key-Value 集群Zookeeper追求的是一致性，但不保证强一致性（半数Follower返回ACK），而是保证顺序一致性（依靠事务ID的单调递增） 集群角色Zookeeper集群中的节点有三种角色 Leader：处理集群的所有事务请求，集群中只有一个Leader Follower：只能处理读请求，可以参与Leader选举 Observer：只负责读，提升集群的读性能，不参与Leader选举 集群搭建1234567891011121314151617181920212223242526# 1. 在/usr/local/zookeeper下创建四个文件，分别用来保存四个节点的数据# 分别保存4个节点的myid，并赋值/usr/local/zookeeper/zkdata/zk1 # echo 1 &gt; myid/usr/local/zookeeper/zkdata/zk2 # echo 2 &gt; myid/usr/local/zookeeper/zkdata/zk3 # echo 3 &gt; myid/usr/local/zookeeper/zkdata/zk4 # echo 4 &gt; myid# 2. 编写4个zoo.cfg# 举例在zk2中，配置文件为zoo2.cfg# dataDir = /usr/local/zookeeper/zkdata/zk2# client_port = 2182# 添加4个节点的通信端口和选举端口# server.1=xxx.xxx.xxx.xxx:2001:3001# server.2=xxx.xxx.xxx.xxx:2002:3002# server.3=xxx.xxx.xxx.xxx:2003:3003# server.4=xxx.xxx.xxx.xxx:2004:3004:observer # 声明为Observer不参与选举# 3. 启动集群./zkServer.sh start ../conf/zoo1.cfg./zkServer.sh start ../conf/zoo2.cfg./zkServer.sh start ../conf/zoo3.cfg./zkServer.sh start ../conf/zoo4.cfg# 4. 查看节点角色./zkServer.sh status ../conf/zoo1.cfg... 集群连接12./bin/zkCli.sh -server xxx.xxx.xxx.xxx:2181,xxx.xxx.xxx.xxx:2182,xxx.xxx.xxx.xxx:2183# 连接所有集群节点的client_port ZAB协议ZAB（Zookeeper Atomic Broadcast）协议，解决了Zookeeper的崩溃恢复和主从数据同步的问题。 状态ZAB定义了四种节点状态 Looking：选举状态 Following：Follower节点所处的状态 Leading：Leader节点所处的状态 Observing：Observer节点所处的状态 启动时的选举Zookeeper集群的节点在上线时，会进入到Looking状态，当启动的节点大于1台时，就开始进入选举流程 节点1生成自己的选票，选票格式为（myid | zXid），其中myid为配置的节点id，zXid为节点的事务（增删改）执行次数，刚上线时，zXid&#x3D;0，此时节点1生成的选票为（1 | 0） 节点2生成自己的选票（2 | 0） 节点1和节点2分别将自己的选票发送给对方，此时两个节点中都存在两张选票：（1 | 0）、（2 | 0） 每个节点依次比较所有选票的zXid、myid，将较大的那张选票投入到投票箱中，这个步骤结束后，两个节点的投票箱中存在1张选票，选票为（2 | 0） 由于配置文件中定义了参与选举的节点数为3，所以只有当投票箱中有节点选票过半（3&#x2F;2 &#x3D; 1.5），即获得两张选票时，才能选出Leader，于是开始第二轮投票 第二轮开始，每个节点会将手上较大的选票发送给对方，而非第一轮时自己的选票，发送后两个节点中都存在两张选票：（2 | 0）、（2 | 0），其中一张是自己生成的，另一张是对方投递过来的 经过比较两张选票，将较大的投入到投票箱中，这个步骤结束后，两个节点的投票箱中都存在2张选票，选票为（2 | 0）、（2 | 0） myid为2的节点获票过半，则被选为Leader 此时启动节点3，如果集群已经产生Leader，则节点3会自动将自己作为Follower 崩溃时的选举Leader建立完成后，会周期性向Follwer发送心跳（Ping命令，没有内容的socket） 当Leader崩溃后，Follower发现socket通道已关闭，于是Follwer会进入到Looking状态，重新回到Leader选举状态，此时集群不能对外提供服务 数据同步客户端向服务端写数据时，数据会首先传给Leader节点，由Leader节点处理数据的写入 Leader节点先把数据写到自己的数据文件中，并给自己返回一个ACK Leader把数据发送给所有的Follower Follwer将数据写到本地数据文件中，并给Leader返回一个ACK Leader收到半数以上的ACK后向所有的Follower发送Commit Follwer收到Commit后把数据文件中的数据写到内存中 NIO和BIOBIO，Block-IO是一种同步阻塞的通信模式，NOI，即Non-Block IO，是一种非阻塞同步的通信模式 除此之外还有AOI，Asynchronous IO，异步非阻塞通信模式 BIO适用于连接数目较小且固定的架构，这种方式对服务器资源要求较高，并发局限于应用中 NIO适用于连接数目多且连接比较短的架构，并发局限于应用中 AIO适用于连接数目多且连接比较长的架构，充分调用OS参与并发操作 NIO用于被客户端连接的2181端口，使用的是NIO模式与客户端建立连接 客户端开启Watch时，也是NIO，等待Zookeeper服务器的回调 BIO集群在选举时，多个节点之间的投票通信使用的是BIO CAP理论一个分布式系统最多只能满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项 一致性Consistency，指的是all nodes see the same data at the same time，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致 可用性Availability，指的是Reads and writes always succeed，即服务一致可用，且是正常相应时间 分区容错性Partition tolerance，指的是the system continues to operate despite arbitrary message loss or failure of part of the system，即分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性或可用性的服务 BASE理论BASE理论是CAP理论的延伸，核心思想是即使无法做到强一致性（Strong Consistency），但应用可以采用适合的方法达到最终一致性（Eventual Consistency） 基本可用Basically Available，指的是分布式系统在出现故障的时候，允许损失部分可用性，保证核心可用 软状态Soft State，指的是允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有三个副本，允许不同节点间副本同步的延迟就是软状态的体现 最终一致性Eventual Consistency，指的是系统中的所有数据副本经过一定时间后，最终能够达到一致的状态 CuratorCurator是Netflix公司开源的一套Zookeeper客户端框架，封装了大部分Zookeeper的功能，比如Leader选举、分布式锁等，减少了技术人员再使用Zookeeper时的底层细节开发工作 在Java开发时引入依赖即可调用对应的方法进行操作","categories":[{"name":"Data","slug":"Data","permalink":"https://dangyoo.github.io/categories/Data/"},{"name":"Integration","slug":"Data/Integration","permalink":"https://dangyoo.github.io/categories/Data/Integration/"},{"name":"Zookeeper","slug":"Data/Integration/Zookeeper","permalink":"https://dangyoo.github.io/categories/Data/Integration/Zookeeper/"}],"tags":[{"name":"知识","slug":"知识","permalink":"https://dangyoo.github.io/tags/%E7%9F%A5%E8%AF%86/"}]},{"title":"微信公众号开发","slug":"微信公众号开发","date":"2023-08-20T07:17:10.000Z","updated":"2024-03-31T08:42:18.643Z","comments":true,"path":"2023/08/20/微信公众号开发/","link":"","permalink":"https://dangyoo.github.io/2023/08/20/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E5%BC%80%E5%8F%91/","excerpt":"闲言碎语大概在18年的时候曾经开过一个公众号，用了当时比较火的小黄鸭做了一个人工聊天机器人，后来慢慢就荒废了，近来想着可以做一些技术的应用实践，就把这个公众号捡起来。","text":"闲言碎语大概在18年的时候曾经开过一个公众号，用了当时比较火的小黄鸭做了一个人工聊天机器人，后来慢慢就荒废了，近来想着可以做一些技术的应用实践，就把这个公众号捡起来。 第一步：服务配置首先需要将自己的服务器和微信公众号平台进行对接，接入指南 基本逻辑是，现在服务器上配置一个服务，对应一个接口，来响应微信这边发出的配置请求 微信发出的请求里包含了signature、timestamp、nonce、echostr这四个东西，需要在服务端获取他们，然后做一个计算，返回一个东西 微信这边判断返回的东西和想要的一样，那就对接上了 用Flask框架搭个服务首先肯定是配置环境，看到网上之前收藏的攻略用的是web.py，但这东西不支持python2，所以就换一个成熟一点的 1234conda create --name gzh python=3.9 # 有些包对3.10的支持不太好，3.9会稳一点# 创建一个gzh开发所用的虚拟环境conda activate gzh # 激活虚拟环境pip install flask # 安装flask包 安装好后开始编写一个响应Web请求的应用 12345678910111213141516171819202122232425262728293031# wechat.py# coding:utf-8from flask import Flask, request, abortimport hashlibWECHAT_TOKEN = &#x27;xxx&#x27; # 随便写，后边与公众号基本配置中配置的token一致即可app = Flask(__name__)@app.route(&quot;/wechat&quot;) # 响应路径def wechat(): signature = request.args.get(&#x27;signature&#x27;) # 从请求中获取相应的变量值 timestamp = request.args.get(&#x27;timestamp&#x27;) nonce = request.args.get(&#x27;nonce&#x27;) echostr = request.args.get(&#x27;echostr&#x27;) if not all([signature, timestamp, nonce, echostr]): abort(400) # 如果没有则返回400响应 temp = [WECHAT_TOKEN, timestamp, nonce] # 按要求生成列表 temp.sort() # 按要求排序 temp = &#x27;&#x27;.join(temp) sign = hashlib.sha1(temp.encode(&#x27;utf8&#x27;)).hexdigest() # 按要求加密 if signature != sign: abort(403) # 如果不一致则返回403响应 else: return echostr # 按要求返回if __name__ == &#x27;__main__&#x27;: app.run(host=&#x27;0.0.0.0&#x27;, port=80, debug=True) # 不写host可能外网无法访问 在wechat.py所在路径下执行sudo python3 wechat.py 后台执行则nohup sudo python3 wechat.py &amp; 要注意的点是sudo python3和虚拟环境下的python3对应的sys.path不同问题需要处理 1234567891011121314151617# 如果需要用sudo python3来执行，比如flask，就把python3的路径写到sudo python3的路径底下# python3命令的执行路径# python3 -&gt; import sys -&gt; sys.path[&#x27;&#x27;, &#x27;/home/xz/miniconda3/envs/gzh/lib/python3.9/site-packages&#x27;] # sudo python3命令的执行路径# sudo python3 -&gt; import sys -&gt; sys.path[&#x27;&#x27;, &#x27;/usr/lib/python3/dist-packages&#x27;]# 将python3命令的路径内容放到一个.pth文件中vim userpathonpath.pth# /home/xz/miniconda3/envs/gzh/lib/python3.9/site-packages# 一行一个把python3的路径写到文件里，一行一个，不需要引号# 然后把文件copy到sudo python3的路径下sudo cp userpathonpath.pth /usr/lib/python3/dist-packages/userpathonpath.pth# 这样sudo python3在寻找路径的时候就会遍历.pth文件中的路径# sudo python3 -&gt; import sys -&gt; sys.path[&#x27;&#x27;, &#x27;/usr/lib/python3/dist-packages&#x27;, &#x27;/home/xz/miniconda3/envs/gzh/lib/python3.9/site-packages&#x27;] 另外公众号端也需要一些配置 123456公众号左侧目录中找到“配置与开发”- “基本配置”URL填写你自己的服务器IP和响应路径，例如http://xxx.xxx.xxx.xxx/wechatToken填写服务端配置好的WECHAT_TOKENEncodingAESKey点一下随机生成消息加解密方式选择兼容模式提交，返回“配置成功”即可 收发文字消息最基本的就是接收消息和回复消息了，官方文档 消息的结构是xml，使用Python处理xml结构的数据可以使用xmltodict包pip install xmltodict 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# -*- coding: utf-8 -*-from flask import Flask, request, make_responseimport hashlibimport xmltodictimport timeapp = Flask(__name__)# 接收消息的是POST，GET只用于服务配置@app.route(&#x27;/wechat&#x27;,methods=[&#x27;GET&#x27;,&#x27;POST&#x27;])def index(): # 这部分是服务配置的处理 if request.method ==&#x27;GET&#x27;: token = &#x27;xxx&#x27; data = request.args signature = data.get(&#x27;signature&#x27;) timestamp = data.get(&#x27;timestamp&#x27;) nonce = data.get(&#x27;nonce&#x27;) echostr = data.get(&#x27;echostr&#x27;) temp = [timestamp, nonce, token] temp.sort() temp = &#x27;&#x27;.join(temp) if (hashlib.sha1(temp.encode(&#x27;utf8&#x27;)).hexdigest() == signature): return echostr else: return &#x27;error&#x27;, 403 # 这部分是消息收发处理 if request.method == &#x27;POST&#x27;: # 获取微信服务器post过来的xml数据 xml = request.data # 用xmltodict包处理xml格式的数据，转换成字典进行取值 req = xmltodict.parse(xml)[&#x27;xml&#x27;] # 判断post过来的数据中数据类型是不是文本 if &#x27;text&#x27; == req.get(&#x27;MsgType&#x27;): # 获取用户的消息，开始构造返回数据，把用户发送的消息原封不动的返回过去 # 先构造为字典格式 resp = &#123; &#x27;ToUserName&#x27;:req.get(&#x27;FromUserName&#x27;), &#x27;FromUserName&#x27;:req.get(&#x27;ToUserName&#x27;), &#x27;CreateTime&#x27;:int(time.time()), &#x27;MsgType&#x27;:&#x27;text&#x27;, &#x27;Content&#x27;:req.get(&#x27;Content&#x27;) &#125; else: # 如果发过来的是图片或者其他非文本的格式，则返回无法识别 resp = &#123; &#x27;ToUserName&#x27;: req.get(&#x27;FromUserName&#x27;, &#x27;&#x27;), &#x27;FromUserName&#x27;: req.get(&#x27;ToUserName&#x27;, &#x27;&#x27;), &#x27;CreateTime&#x27;: int(time.time()), &#x27;MsgType&#x27;: &#x27;text&#x27;, &#x27;Content&#x27;: &#x27;无法识别该消息类型&#x27; &#125; # 把构造的字典转换成xml格式 xml = xmltodict.unparse(&#123;&#x27;xml&#x27;:resp&#125;) # 返回数据 return xmlif __name__ == &#x27;__main__&#x27;: app.run(host=&#x27;0.0.0.0&#x27;, port=80, debug=True) 到这里基本的收发信息就处理好了，其实也就足以做一些基本功能的开发了 Nginx相关由于Nginx默认端口是80，如果不做配置的话，这个服务因为端口冲突就会无法启动，后续把Nginx搞明白再做端口变更，现在先去Nginx目录下停止服务 1234whereis nginx# /usr/sbin/nginxcd /usr/sbin/./nginx -s stop 第二步：读写数据库实现这样一个需求： 用户发送一个电影名字，公众号返回一个资源链接 资源存储在服务器的一个数据库中 管理员可以发送特殊格式的消息来向数据库中写入数据 需求细化 判断用户消息是否文本 用，分割后，判断第一部分是否su 如果是su，则第二部分为name，第三部分为url，不需要返回 如果不是su，则第二部分为name，需要查询，然后返回url PostgreSQL配置1234567891011121314151617181920212223242526272829sudo apt-get updatesudo apt-get install postgresql postgresql-client# 安装好后会默认创建postgres用户sudo -u postgres -i # 进入postgres这个用户权限下psql # 登入PostgreSQL# SELECT * FROM pg_user; # 可以查看所有用户的信息# CREATE USER &lt;username&gt; superuser; # 创建新用户并且给超级管理权限# ALTER USER &lt;username&gt; superuser; # 给对应用户超级管理权限# ALTER USER &lt;username&gt; WITH PASSWORD &#x27;xxx&#x27;; # 修改密码# CREATE DATABASE &lt;databasename&gt;; # 创建数据库# 切换到对应与用户下创建表# CREATE TABLE movie_info(# name TEXT,# url TEXT# );# \\d 查看已经建好的表# \\q 退出pip install psycopg2 # 安装Python操作PostgreSQL包# 如果出现ERROR: Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects# 则执行sudo apt-get install libpq-dev python3-dev# 安装后在python环境导入import psycopg2测试# 如果出现ImportError: no pq wrapper available则执行pip install &quot;psycopg[binary,pool]&quot;# 如果出现undefined symbol: ffi_type_pointer, version LIBFFI_BASE_7.0则执行conda install libffi==3.3 修改脚本连接PostgreSQL 1234567891011121314import psycopg2conn = psycopg2.connect(database=&#x27;x&#x27;,user=&#x27;x&#x27;,password=&#x27;x&#x27;,host=&#x27;127.0.0.1&#x27;,port=&#x27;5432&#x27;)cursor = conn.cursor()# 查询cursor.execute(&#x27;SELECT * FROM x&#x27;)result = cursor.fetchall()print(result)# 插入cursor.execite(&quot;&quot;&quot;INSERT INTO x(col1, col2) VALUES(&#x27;x&#x27;, &#x27;x&#x27;)&quot;&quot;&quot;)conn.commit()conn.close() # 关闭连接 服务脚本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# -*- coding: utf-8 -*-from flask import Flask, request, make_responseimport hashlibimport xmltodictimport timeimport psycopg2app = Flask(__name__)@app.route(&#x27;/wechat&#x27;,methods=[&#x27;GET&#x27;,&#x27;POST&#x27;])def index(): if request.method ==&#x27;GET&#x27;: # 设置token,开发者配置中心使用 token = &#x27;x&#x27; # 获取微信服务器发送过来的参数 data = request.args signature = data.get(&#x27;signature&#x27;) timestamp = data.get(&#x27;timestamp&#x27;) nonce = data.get(&#x27;nonce&#x27;) echostr = data.get(&#x27;echostr&#x27;) # 对参数进行字典排序，拼接字符串 temp = [timestamp, nonce, token] temp.sort() temp = &#x27;&#x27;.join(temp) # 加密 if (hashlib.sha1(temp.encode(&#x27;utf8&#x27;)).hexdigest() == signature): return echostr else: return &#x27;error&#x27;, 403 # 根据请求方式进行判断 if request.method == &#x27;POST&#x27;: # 获取微信服务器post过来的xml数据 xml = request.data # 把xml格式的数据进行处理，转换成字典进行取值 req = xmltodict.parse(xml)[&#x27;xml&#x27;] # 判断post过来的数据中数据类型是不是文本 if &#x27;text&#x27; == req.get(&#x27;MsgType&#x27;): # 连接数据库 conn = psycopg2.connect(database=&#x27;x&#x27;,user=&#x27;x&#x27;,password=&#x27;x&#x27;,host=&#x27;127.0.0.1&#x27;,port=&#x27;5432&#x27;) cursor = conn.cursor() # 判断内容是要执行插入还是查询 content = req.get(&#x27;Content&#x27;) content_list = content.split(&#x27;，&#x27;) if content_list[0] == &#x27;x&#x27;: # 插入信息 cursor.execute(&quot;&quot;&quot;INSERT INTO x(x, x) VALUES(&#x27;%s&#x27;, &#x27;%s&#x27;)&quot;&quot;&quot;%(content_list[1], content_list[2])) conn.commit() result = &#x27;写入完成&#x27; else: # 查询信息 cursor.execute(&quot;SELECT x FROM x WHERE x = &#x27;%s&#x27;&quot;%content_list[0]) result = cursor.fetchall() if result == []: result = &#x27;xxx&#x27; else: result = result[0][0] # 获取用户的信息，开始构造返回数据，把用户发送的信息原封不动的返回过去，字典格式 resp = &#123; &#x27;ToUserName&#x27;:req.get(&#x27;FromUserName&#x27;), &#x27;FromUserName&#x27;:req.get(&#x27;ToUserName&#x27;), &#x27;CreateTime&#x27;:int(time.time()), &#x27;MsgType&#x27;:&#x27;text&#x27;, &#x27;Content&#x27;:result &#125; # 把构造的字典转换成xml格式 xml = xmltodict.unparse(&#123;&#x27;xml&#x27;:resp&#125;) # 返回数据 return xml else: resp = &#123; &#x27;ToUserName&#x27;: req.get(&#x27;FromUserName&#x27;, &#x27;&#x27;), &#x27;FromUserName&#x27;: req.get(&#x27;ToUserName&#x27;, &#x27;&#x27;), &#x27;CreateTime&#x27;: int(time.time()), &#x27;MsgType&#x27;: &#x27;text&#x27;, &#x27;Content&#x27;: &#x27;无法识别该消息类型&#x27; &#125; xml = xmltodict.unparse(&#123;&#x27;xml&#x27;:resp&#125;) return xmlif __name__ == &#x27;__main__&#x27;: app.run(host=&#x27;0.0.0.0&#x27;, port=80, debug=True)","categories":[{"name":"小项目","slug":"小项目","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/"},{"name":"微信公众号","slug":"小项目/微信公众号","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7/"}],"tags":[{"name":"实践","slug":"实践","permalink":"https://dangyoo.github.io/tags/%E5%AE%9E%E8%B7%B5/"}]},{"title":"梯子搭建攻略","slug":"梯子搭建攻略","date":"2023-06-30T08:17:16.000Z","updated":"2024-03-31T08:40:03.195Z","comments":true,"path":"2023/06/30/梯子搭建攻略/","link":"","permalink":"https://dangyoo.github.io/2023/06/30/%E6%A2%AF%E5%AD%90%E6%90%AD%E5%BB%BA%E6%94%BB%E7%95%A5/","excerpt":"闲言碎语最近搞了一台海外节点的LightHouse，想着搞点东西，先搭一个梯子。又想到自从会上网以来，对VPN、VPS、Proxy、Shadowsocks等词都是大概知道意思，对于其中的逻辑没有系统了解过，遂了解一下。","text":"闲言碎语最近搞了一台海外节点的LightHouse，想着搞点东西，先搭一个梯子。又想到自从会上网以来，对VPN、VPS、Proxy、Shadowsocks等词都是大概知道意思，对于其中的逻辑没有系统了解过，遂了解一下。 翻墙原理GFW实现网络封锁的手段主要有两种：DNS劫持和IP封锁（以及DNS污染和关键词过滤）： DNS劫持：DNS（Domain Name System）是负责将域名和IP地址映射起来的服务，而GFW所做的就是在用户和DNS服务之间，破坏他们的正常通讯，并向用户回传一个假的IP，用户拿不到真实的IP，自然就访问不到本想访问的网站了。所以早期有一些翻墙方式，是修改hosts文件，将域名对应的真实IP添加到文件中，来避免这种劫持。 IP封锁：指的是直接锁住对应的IP去路，也就是用户即使知道了真实IP，发往被封锁IP的所有数据都会被截断，此时，修改hosts文件这种方式就不行了。解决方案就是在第三方假设翻墙服务器，用来中转目标服务器之间的来往流量。目前为止，GFW采用的是黑名单模式，像Google这种在黑名单里的网站IP无法访问，而不在黑名单上的第三方不记名IP则可以。 VPN全称虚拟私人网络（Virtual Private Network），是一种加密通讯技术。VPN是一个统称，他有很多实现方式，比如PPTP（点对点隧道协议Point to Point Tunneling Protocol）、L2TP（第二层隧道协议Layer Two Tunneling Protocol）、IPSec（互联网安全协议Internet Protocol Security）和openvpn等。 VPN的出现早于GFW，所以它不是为了翻墙而生，它只是一种加密通讯技术，设计目的是数据传输安全和网络匿名。 既然不是专为翻墙而生，那从翻墙的角度讲，VPN协议就存在诸多问题，最严重的一个就是流量特征过于明显，GFW目前已经能够精确识别绝大部分VPN协议的流量特征并给予封锁，所以，VPN这种翻墙方式基本已经废了。 即便如此，VPN作为过去很长一段时间最主流最热门最常用最为人所知的翻墙手段，已然成为翻墙的代名词。即便是VPN已经不再常用的今天，当人们谈及翻墙时，还是会说：“有啥好用的VPN吗？” Proxy代理，分为正向代理和反向代理。翻墙所用的代理都是正向代理，反向代理的作用主要是为服务器做缓存和负载均衡。 正向代理主要协议有HTTP、HTTP over TLS（HTTPS）、Socks、Socks over TLS几种，其中，HTTP和Socks无法 用于翻墙，HTTPS和Socks over TLS可以用于翻墙。常用的还是HTTPS的方式。 Proxy的历史同样早于GFW，所以也不是为了翻墙而生，与VPN差不多，都是为了匿名，但HTTP和Socks不能加密，只能匿名，HTTPS既可以匿名，也可以用于加密通信。 理论上将，四种代理协议都可以通过“用户将数据先发送给代理服务器，再由代理服务器转发给目标服务器”的方法实现翻墙。但由于HTTP和Socks都是明文协议，GFW完全可以通过检查数据包内容得知用户的真实意图，从而拦截。所以一般只用做本地代理。而HTTPS协议是加密通讯，GFW无法得知数据包内的真是内容，类似于关键词过滤的手段便无法开展。 VPS虚拟专用服务器（Virtual Private Server），是由提供商维护，租用给个人的“不会关机”的电脑。VPS不是独立的电脑，而是将一台巨型服务器通过虚拟化技术分割成若干台看似独立的服务器。这台巨型服务器不间断运行，被分割出来的小服务器也跟着不停运转，个人租用其中一台小服务器，搭载上自己的服务即可。 所以不管是腾讯的CVM、阿里的ECS，本质上都是VPS服务。 ShadowsocksShadowsocks同样是一种代理协议，但是作为@clowwindy专为国人设计的专用翻墙协议，相对于VPN，Shadowsocks具有极强的隐匿性，相对于HTTP代理，Shadowsocks提供了较为完善的加密方案。虽然比不上HTTPS，但使用的也是成熟的工业级加密算法，而相对于HTTPS代理的复杂配置，Shadowsocks的安装配置更为简单，中文社区更为活跃，于是成为了现在最常见的翻墙手段。 V2RayV2Ray是一个平台，所有人都可以基于这个平台开发自己的翻墙工具，它支持了多种协议，其中VMess协议是V2Ray社区原创的加密协议。 配置V2RayV2Ray本身有大量的第三方开发客户端，这里选其中较为方便配置的一种。 系统要求Ubuntu 16+ &#x2F; Debian 9+ &#x2F; CentOS 7+ 代码123456789101112131415161718# 使用root用户进行安装bash &lt;(curl -s -L https://git.io/v2ray.sh) # 这是一个一键安装脚本# 如果提示curl command not found，则需要先安装curlapt-get update -y &amp;&amp; apt-get install curl -y # ubuntu/debianyum update -y &amp;&amp; yum install curl -y # centOS# 安装完之后，输入v2ray即可进入管理v2ray add ss # 添加一个Shadowsocks配置# 正常添加之后会生成一个配置如下# -------------- Shadowsocks-xxx.json -------------# 协议 (protocol) = shadowsocks# 地址 (address) = xxx.xxx.xxx.xxx# 端口 (port) = xxx# 密码 (password) = xxx# 加密方式 (encryption) = chacha20-ietf-poly1305# ------------- 链接 (URL) -------------# ss://xxxxx# 如果已有Shadowrocket等Shadowsocks客户端，添加对应URL或者配置信息即可","categories":[{"name":"小项目","slug":"小项目","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/"},{"name":"搭梯子","slug":"小项目/搭梯子","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E6%90%AD%E6%A2%AF%E5%AD%90/"}],"tags":[{"name":"实践","slug":"实践","permalink":"https://dangyoo.github.io/tags/%E5%AE%9E%E8%B7%B5/"}]},{"title":"Erupt后台管理平台搭建","slug":"Erupt后台管理平台搭建","date":"2023-03-30T08:17:16.000Z","updated":"2024-03-31T08:42:18.647Z","comments":true,"path":"2023/03/30/Erupt后台管理平台搭建/","link":"","permalink":"https://dangyoo.github.io/2023/03/30/Erupt%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0%E6%90%AD%E5%BB%BA/","excerpt":"闲言碎语最近工作中需要实现一个简单的表单式呈现工具，主要用来展现一些元数据、业务流程数据，然后管理一些表和业务流程的关系等 主要需要的能力是能够做表单展示、能够编辑内容、能够数据互通（非手动） 本来考虑的是用鹅的微搭，结果搞了半天发现这东西主要是用来快速实现小程序的，其他东西就是做了个样子，数据导入导出API？那是什么东西？ 所以考虑其他工具，之前用过明道云，算是比较成熟且好用的产品了，可惜要花钱，不是我能决定的 之前搭过Django后台管理，实现过一些需求，不过这次领导推荐了Erupt，说自己玩过几次，那我也来玩玩","text":"闲言碎语最近工作中需要实现一个简单的表单式呈现工具，主要用来展现一些元数据、业务流程数据，然后管理一些表和业务流程的关系等 主要需要的能力是能够做表单展示、能够编辑内容、能够数据互通（非手动） 本来考虑的是用鹅的微搭，结果搞了半天发现这东西主要是用来快速实现小程序的，其他东西就是做了个样子，数据导入导出API？那是什么东西？ 所以考虑其他工具，之前用过明道云，算是比较成熟且好用的产品了，可惜要花钱，不是我能决定的 之前搭过Django后台管理，实现过一些需求，不过这次领导推荐了Erupt，说自己玩过几次，那我也来玩玩 本地跑Demo第一步肯定是先在本地搭一个，然后再去服务器上搭（谁成想领导说你怎么不用Docker，要什么服务器） 官方文档 跟着官方的部署步骤进行操作 因为后续正式使用是用的MySQL，所以首先在本地装好MySQL，版本影响不大，但是8.0和常见5.0的语法不太一样MySQL安装，新建一个database以供项目使用，示例中用的是erupt 在Spring Initializr创建一个项目。这里左侧Project选择Maven，Language选择Java，Spring Boot选择2.7.12（暂时3.x的支持没有2.x的好），Java版本选17，然后下方选择GENERATE生成一个demo.jar压缩包并下载 解压缩，在IDEA中打开为Maven工程，这一步如果操作错了导致打开后IDEA右侧没有Maven选项，可以参考本文进行补救 有几个关键的文件，一个是pom.xml，在里边添加依赖包，例如erupt的UI页面包、用户权限包，或者Java连接MySQL的依赖包等；另一个是resources文件夹，里边存的是后面会用到的配置信息、静态文件等等资源 根据官方文档在pom.xml添加所需的依赖包，将示例中的$&#123;erupt.version&#125;改为具体的版本号如1.11.7 1234567&lt;!--例如一个MySQL5的依赖包--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.49&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 修改后会在IDEA右上角出现一个m字符的图标，点击Load Maven Changes，可以下载对应依赖包，快捷键是Command + Shift + I 根据官方文档在resources文件夹下新建public文件夹，并在其中添加app.js，app.css，home.html文件，按照示例（点击官方文档表格后边的#）复制内容进去 根据官方文档在resources文件夹下新建application.yml文件（properties应该也行但是我没成功），注意数据库链接中，示例的jdbc:mysql://127.0.0.1:3306/erupt中的erupt是事先建好的database 根据官方文档在Spring Boot入口类，也就是demo.src.main.java.com.example.demo.DemoApplication中，添加 @EntityScan、@EruptScan 注解，添加后如果IDEA中显示红色，则需要引入，鼠标放在红色上点击Import class即可，或者快捷键Alt + Shift + Enter 12import org.springframework.boot.autoconfigure.domain.EntityScan;import xyz.erupt.core.annotation.EruptScan; 运行DemoApplication，可以选择在左侧目录树右键选择Run，也可以在文件中运行main方法 启动后会自动在对应的database中新建一些表，然后在http://localhost:8080可以看到登录页，默认用户名密码都是erupt 过程中遇到什么问题优先查看官方Q&amp;A，搜索引擎搜到对应内容挺不容易，也可以考虑去B站看视频操作 数据表的创建按照官方入门示例开始通过创建Class来给数据库建表，并通过管理菜单绑定到页面上 在入口类同级别创建model文件夹，也就是demo.src.main.java.com.example.demo.model，用来存放所有会用到的Class，一个Class对应一个可以在管理菜单进行绑定的类型值 一些可能用到的语法，view和edit的各类配置项参考官方文档以及示例1234567private Long id; # 字段id的类型为Long@Lobprivate String comment; # 字段comment的类型为textviews = @View(title = &quot;名称&quot;, sortable = true) # 字段可以点击表头进行排序edit = @Edit(title = &quot;名称&quot;, readonly = @Readonly, search = @Search(vague = true)) # 字段不可编辑，可以进行模糊搜索（页面上方会有虚线搜索框，false为精确搜索，搜索框为实线） 部署本地把项目创建好并且导入部分数据玩熟了之后，开始考虑部署的问题，本来想着整个内部服务器上去操作以便就行了，但是领导非要让用Docker部署到k8s上，学吧有啥办法 啥是Docker虽然之前听说过这，大概有个印象是能做到环境隔离，然后能快速部署，具体怎么实现，完全不知道，参考文档学了半天基本搞明白怎么操作了 按照个人浅薄但够用的理解来说，Docker整个技术就是将某个应用所需的软件依赖编写在Dockerfile中，通过Docker引擎将所需的依赖打包到一个镜像image中，使得这个镜像可以在任何硬件上运行的过程 咋打包这个erupt项目首先这个项目本身需要打包成一个jar包，为了运行这个jar包，需要一个能够运行java的JDK，然后最底层需要一个linux操作系统 那对于Dockerfile来说，最先应该引入一个linux操作系统镜像，然后引入一个对应版本的JDK镜像，然后将这个项目的jar包导入，然后运行就好了 12345FROM xxx/jdk:17 # 这里先把linux+JDK打包成了一个镜像传到了仓库中MAINTAINER dangyooEXPOSE 8080ADD demo.jar /app.jar # 把本地的项目jar包导入ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] # 运行这个包 在把项目打包成demo.jar的时候，最好使用Maven工具进行，IDEA本身的built会有很多依赖冲突的问题，在打包前最好将测试跳过，否则会连接数据库，如果数据库配置的是内网数据库，就会连不上报错 跳过测试设置：Settings - Build, Execution, Deployment - Build Tools - Maven - Runner - Properties - Skip Tests打勾 使用Maven打包：右上角Maven - demo - Lifecycle - package 打包后默认文件路径是在demo/target/demo-0.0.1-SNAPSHOT.jar这里","categories":[{"name":"小项目","slug":"小项目","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/"},{"name":"低代码平台","slug":"小项目/低代码平台","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E4%BD%8E%E4%BB%A3%E7%A0%81%E5%B9%B3%E5%8F%B0/"}],"tags":[{"name":"实践","slug":"实践","permalink":"https://dangyoo.github.io/tags/%E5%AE%9E%E8%B7%B5/"}]},{"title":"农历和阴历是啥关系","slug":"农历和阴历是啥关系","date":"2023-03-29T02:20:41.000Z","updated":"2024-03-31T08:00:48.905Z","comments":true,"path":"2023/03/29/农历和阴历是啥关系/","link":"","permalink":"https://dangyoo.github.io/2023/03/29/%E5%86%9C%E5%8E%86%E5%92%8C%E9%98%B4%E5%8E%86%E6%98%AF%E5%95%A5%E5%85%B3%E7%B3%BB/","excerpt":"闲言碎语快要清明节了，昨天闲聊突然想到，为啥清明节是阳历的4月5日，而其他很多传统节日又是阴历，比如七月初七啥的，按照未曾证实过的固有印象，中国古代用的农历就是现在的阴历，所以中国传统节日不都应该是阴历吗？ 遂查资料","text":"闲言碎语快要清明节了，昨天闲聊突然想到，为啥清明节是阳历的4月5日，而其他很多传统节日又是阴历，比如七月初七啥的，按照未曾证实过的固有印象，中国古代用的农历就是现在的阴历，所以中国传统节日不都应该是阴历吗？ 遂查资料 为什么清明节是阳历清明节本来并不是传统节日，而是属于二十四节气之一，慢慢变成了节日，而二十四节气是依据古代的阳历划分的 节气 节气指二十四时节和气候，是中国古代用来指导农事之历法历注 中国传统夏历（农历）是一种“阴阳合历”，同时根据日、月运行制定，“阴”是以朔望月为基准确定，“阳”是以地球自冬至绕太阳公转一圈为基准确定岁实，每回归年约365.2422日，二十四节气据此而划分 所以古代的阳历是以地球公转周期（和太阳相关）制定的，而阴历是以月相制定的。由于太阳和农业之间的密切关系，阳历用来指导农业生产，确实理所应当。 古人将黄道划分4季，12节，24气，72候。定义3节为一季，约30日为一节，15日为一气，5日为一候。 清明：清明之日桐始华，又五日田鼠化为鴽，又五日虹始见。 清明是怎么从节气变为节日的 清明 传统寒食节、上巳节、清明节相距甚近，为了方便，往往连续假期，唐代时寒食与清明已并称，清明本无太多文化内涵，而寒食、上巳则传统丰厚，三相结合，便成了一个重要的节日。 寒食节 通常是冬至后第105日，与清明节日期相近，在清明节前一或二日。清初汤若望《时宪历》订定后，清明与冬至之间的间隔缩短，为了维持寒食节在清明节前一、两日的风俗，民间将寒食节定在清明节一日之前。现代24节气的定法沿袭汤氏，因此清明节就在寒食节次日。 一般认为寒食节是为了纪念介子推，寒食节的真正起源，是源于古代的钻木、求新火之制。古人因季节不同，用不同的树木钻火，有改季改火之俗。而每次改火之后，就要换取新火。新火未至，就禁止人们生火，这在当时是件大事。 据敦煌文献“进奏院状”载，晚唐时沙州赴京请旌节者称：“五日遇寒食，至八日假开。”即寒食清明休4天假。 上巳节 在汉代以前定为三月上旬的巳日，后来固定在农历三月初三。 相传三月三是黄帝、玄天上帝诞辰，也是哪吒太子升天日，还是中国神话中高媒神诞辰，高媒是媒人之神，可保佑媒人，亦可保佑男女恋爱顺利。高媒通说女娲，但尚有简狄、伏羲、勾芒等说法。 所以清明的扫墓是继承的寒食节习俗，踏青是继承的上巳节习俗，怎么没继承休假4天习俗呢。 公历？阳历？农历？阴历？农历属于阴阳合历，其中阴历根据月相制定，阳历根据太阳运行制定，并不是曾以为的农历&#x3D;阴历 阴阳合历 纯“阳历”是指历法中只保证一年的时间与地球绕日运行周期基本一致，不考虑月份，例如华夏24节气；纯“阴历”是指历法中只保证一个月的时间与月亮运行周期基本一致，不考虑年长；而阴阳历则既保证“多年的平均值”与地球绕日周期的一致，又保证“月”与月亮周期的一致。 采用阴阳合历的主要目的是配合季节，因此需要安排闰月来调整，大多数年份有12个朔望月，闰年就会有13个月。连续十九年为一章，其中安置七个闰月置闰，使历年的平均值大约与“地球公转一年”相当（大多采用回归年定义，也有采用恒星年定义者）。构成阴阳合历的历年连续19年一套可名为阴阳章历，四章连续一套为一蔀（详见史记历书），可名为阴阳蔀历。俗称农民历的夏历就是阴阳合历之一种。 因十九年七闰之规律，每个人在其19岁倍数之生日，有机会西历（公历）生日与夏历（农历）重合于同一日 阴阳合历制中单一一历年，其月数、日数不定。 农历 中华民国成立后，孙中山宣布以格里历作官方历法，称为国历、新历，华夏传统历法则返称旧历、传统历、古历。格里历为阳历，而农历是以月相周期安排月份之日期，故习称阴历。 也就是说，虽然农历在古代是一部阴阳合历，但到了民国之后，就叫”阴历”了，这时”阴历”的概念已经完全等于农历，且不等于农历中的”阴历”了 公历，也就是现在俗称的”阳历”，清朝称为”西历”，民国临时政府成立后对外改称”公历” 公历 公历，是当前国际通用的历法，也叫格里历。 1582年10月，教宗格勒哥里十三世介绍和引入这种历法，作为对儒略历的修改和替代。主要的变化是对闰年的加入进行了不同的划分，使平均行事历年长365.2425天，更接近由地球绕太阳公转决定的365.2422天的回归或太阳年。 总结农历是阴阳合历，农历中的阳历和当前的公历不完全相等，但都和太阳运行有关，所以按照农历中阳历制定的二十四节气和当前的公历相关，清明属于二十四节气融合了寒食节、上巳节后演变成的节日，与农历中的阳历相关，也就呈现出与公历相关的样子 农历中的阴历和当前的阴历不等，当前的阴历与农历相等，二十四节气属于农历中的内容，也就属于当前的阴历 其余与节气无关的中国传统节日，如春节、元宵节等，就是源于农历中的阴历部分，所以与当前的公历无关","categories":[{"name":"日常考据","slug":"日常考据","permalink":"https://dangyoo.github.io/categories/%E6%97%A5%E5%B8%B8%E8%80%83%E6%8D%AE/"}],"tags":[{"name":"常识","slug":"常识","permalink":"https://dangyoo.github.io/tags/%E5%B8%B8%E8%AF%86/"}]},{"title":"洋葱在烹饪中的作用","slug":"洋葱在烹饪中的作用","date":"2023-03-17T02:30:20.000Z","updated":"2024-03-31T08:00:48.885Z","comments":true,"path":"2023/03/17/洋葱在烹饪中的作用/","link":"","permalink":"https://dangyoo.github.io/2023/03/17/%E6%B4%8B%E8%91%B1%E5%9C%A8%E7%83%B9%E9%A5%AA%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8/","excerpt":"闲言碎语昨天去吃俄士厨房，上来的几乎每道菜里都有或生或熟的洋葱，我说我不爱吃洋葱，路边炒面都不要洋葱 饭搭子们表示洋葱是烹饪中非常重要的一种食材，能发挥很多总用，但是具体作用是啥为啥发挥，说不出来 遂查资料","text":"闲言碎语昨天去吃俄士厨房，上来的几乎每道菜里都有或生或熟的洋葱，我说我不爱吃洋葱，路边炒面都不要洋葱 饭搭子们表示洋葱是烹饪中非常重要的一种食材，能发挥很多总用，但是具体作用是啥为啥发挥，说不出来 遂查资料 糖的作用模糊有影响之前在哪里看到过说洋葱受热分解可以产生糖，然后通过焦糖反应为食物增加风味 翻了翻之前买的《食物与厨艺：面食·酱料·甜点·饮料》，找到讲糖的那部分： 甜味的复杂作用 甜味是种单纯的感受，糖产生的甜味却不是这样。甜味有助于掩盖或平衡其他成分带来的酸味和枯萎。风味化学家也已经发现，甜味能大幅提升我们对食物香气的感受，甜味也有可能对脑部发出讯号，提示这种食物是优异的能源，有必要特别注意。 不同的糖类会带来不同的甜味印象。舌头要花点时间才能对蔗糖产生反应，尝到的甜味会萦绕多时。相对而言，果糖的甜味很快就能察觉，讯号很强却也很快消退。玉米糖浆的甜味则要很久才能尝出，最甜时，强度约为蔗糖的一般，味道残留时间则比蔗糖更久。据称果糖这种高速反应还能强化食品的其他集中供暖风味，特别是果味、酸味和香料味，这是由于果糖不留余味，不会把其他味道盖掉，而能让我们清楚品尝到。 焦糖化反应 “焦糖化”是指糖类受热超过特定温度，分子开始瓦解产生的化学反应，适用于所有糖类。分子毁损会触发一连串化学反应，产生美妙的产物。厨师从单一种类分子入手，将无色无味的单纯甜味晶体料理成好几百种不同成分的新颖化合物，其中有些是带了酸味、苦味，甚至能散发强烈香气的小碎片；有些则是不带风味，却呈现深褐色的大型凝聚体。糖烧煮越久，残留的甜味越少，颜色越深，味道也越苦。 焦糖化反应产生的风味 糖受热之后，原本没有气味的单糖类甜分子便转化成好几百种不同的分子，散发出繁复风味，展现深褐色泽。其中集中带芳香气味的产物为：酒精、带雪利酒味的乙醛、带醋味的乙酸、带奶油味的醋双乙酰、带果味的乙酸乙酯、带坚果味的呋喃、具溶剂作用的苯、带有烘烤味的麦芽醇。 那如果说洋葱受热产生了糖，就可以用糖的作用来解释洋葱的作用，那接下来就是查一下洋葱是怎么受热产生糖的 洋葱味道的产生查一下洋葱为什么炒了之后会变甜，有些文章提到，洋葱中含有蔗糖，所以是蔗糖的焦糖化反应是风味的一种产出原因，除此之外，也有文章提到，炒洋葱的甜味主要是美拉德反应（又称梅纳反应）造成的 美拉德反应 指的是食物中的还原糖（碳水化合物）与氨基酸／蛋白质在常温或加热时发生的一系列复杂反应，其结果是生成了棕黑色的大分子物质类黑精或称拟黑素。除产生类黑精外，反应过程中还会产生成百上千个有不同气味的中间体分子，包括还原酮、醛和杂环化合物，这些物质为食品提供了宜人可口的风味和诱人的色泽。它以法国化学家路易斯·卡米拉·美拉德命名，他在1912年首次描述它，同时试图重现生物蛋白质合成。美拉德反应的产物中，包含颜色的变黄变深变黑、香气的产生、以及味道上的转变，例如甜味的产生。该反应是一种非酶促褐变的形式，其通常快速从约140摄氏度至170摄氏度中进行。在较高的温度下，焦糖化和随后的裂解变得更加明显。在此过程中，产生了数百种不同的风味化合物。这些化合物又分解成形成更多新的风味化合物等等。 梅納反應跟焦糖化到底差在哪？一次搞懂原理與五種實用範例 美拉德反应除了会产生焦糖化反应的风味之外，还会产生诸如香咸、花香、肉香、巧克力、蔬菜、土豆和泥土味。 虽然洋葱确实含有天然蔗糖，烹煮时有助于产生金黄色泽与甜味，因此也存在焦糖化。但主要的褐变原因还是來自加热蛋白质与还原糖所致。当洋葱丝遇热，表面水分就开始蒸发，颜色渐转焦褐，数以百计的风味分子释出。 美中食材的风味组成差异显著，视各种氨基酸的多寡占比而定。在炒焦化洋葱时，也可以加上些许苏打粉提升PH值，咸性会加强梅纳反应。 什么是美拉德反应? 美拉德反应初期阶段反应包括还原糖的羰基碳首先遭到氨基氮上孤对电子的亲核加成，接着失去水和闭环二形成葡基胺，如果还有过量的还原糖存在，就能进一步形成二葡基胺。葡基胺再经过阿马道里重排而生成1－氨基－2－酮糖。美拉德反应初级阶段不引起褐变，也不产生香味，但其产物是产生极重要的不挥发性香味物质的前驱物。 在氨基酮糖和氨基醛糖等重要的不挥发性香味前驱物形成之后，美拉德反应变得更为复杂，阿马道里重排产物经过－消去机理脱水，在经过脱水脱掉氨基而生成3－脱氧己糖醛酮、奥苏烯糖和HMF 等，这些不同的化合物依次反应，开始形成无氮及含氮褐色可溶性化合物。 高级美拉德反应阶段形成的众多活性中间体如葡萄糖酮醛、3－脱氧、3,4－二脱氧、HMF、二还原酮类、不饱和醛亚胺等等，又可继续与氨基酸反应，最终生成类黑精色素，褐色含氮色素，吡嗪和咪唑环等风味物质。此过程包括醇醛缩合、醛氨聚合、环化合反应等。 在查洋葱时，很多文章都提到了”硫化物”，例如： 为什么西餐离不开洋葱，中餐里却毫无地位，没法和辣椒番茄比？ 洋葱具有辛辣味，是因为它拥有一些含硫物质，但进过充分加热，含硫化合物发生降解，降解之后的丙硫醇就有很好的甜味。所以吃生洋葱是为了辛辣，熟吃则是为了一口清甜味。 丙硫醇具有甜味？在维基百科查到的丙硫醇具有两种异构体，分别是正丙硫醇和异丙硫醇，正丙硫醇是一种无色液体，易挥发，具有臭鼬鼠味，而异丙硫醇是一种闪点易燃的无色液体，具有挥发性，微量具有臭味，对身体有害，大量吸入会引起嗅觉丧失、肌无力等症状。口服引起恶心、呕吐。对眼和皮肤有刺激性。怎么看也和甜味没啥关系。 再搜”硫化物受热降解”，搜到了一篇文章6类常见食品中含硫化合物风味特征及形成机理研究进展 挥发性含硫化合物广泛存在于多种食品中，具有阈值极低的特点，是对食品感官品质有重要贡献的特征风味组分，在形成食品独特风味特征中也起着至关重要的作用。不同种类的含硫化合物表现出的食品风味特征不尽相同，不同含量的含硫化合物对食品感官品质的影响也差异显著，因此，对感官风味受含硫化合物影响较大的6类常见食品中含硫化合物风味特征及其形成机理进行了研究。 二丙基二硫醚、二甲基三硫醚和大蒜素分别为大葱、洋葱和大蒜风味形成的关键含硫化合物 鲜菜类调味品呈现出来的独特味道大多来自于含硫化合物，而含硫化合物是来自于前体物质含硫氨基酸。蒜氨酸是大蒜中甲基烯丙基硫醚等含硫化合物的前体物质，S-烷基-L-半胱氨酸亚砜是洋葱中含硫化合物的前体物质。完整的大蒜是没有气味的，蒜氨酸和蒜氨酸酶分别稳定地处于细胞质和液泡2个不同的细胞器中，当大蒜破碎后蒜氨酸酶被释放，蒜氨酸在蒜氨酸酶的作用下形成大蒜素，随后大蒜素降解为其他的挥发性含硫化合物。洋葱中风味物质的形成与大蒜类似，只有当洋葱被破坏后其中的前体物质S-烷基-L-半胱氨酸亚砜被释放，在蒜氨酸酶的作用下产生洋葱的特征风味物质。 含硫化合物在洋葱中表现出明显的辛辣味和洋葱味，且含硫化合物占洋葱中检测到的挥发性化合物的一半以上。洋葱中主要的含硫化合物有甲硫醇、二丙基二硫醚、二丙基三硫醚、烯丙硫醇、丙硫醇、二甲基二硫醚、二甲基三硫醚、甲基乙基二硫醚、甲基丙基二硫醚、甲基丙烯基二硫醚、1-巯基丙烷、1,3-二噻烷、1,4-二噻烷、2,4-二甲基噻吩、3,4-二甲基噻吩、3-巯基-2-甲基戊醛。王依春等应用固相微萃取和同时蒸馏萃取这2种萃取方法，共检测到52种挥发性化合物，其中含硫化合物占70%左右，主要是一些硫醇和硫醚类的含硫化合物，包括甲硫醇、烯丙硫醇、丙硫醇、二甲基二硫醚、甲基乙基二硫醚、二甲基三硫醚、二丙基二硫醚、二丙基三硫醚等。孙雪君等对不同品种的干洋葱和鲜洋葱中的挥发性化合物进行分析后共检测到16种含硫化合物，主要包括1-巯基丙烷、二甲基三硫醚、甲基丙基二硫醚、二丙基二硫醚、1,3-二噻烷和2,4-二甲基噻吩。研究发现鲜洋葱比干洋葱含有更多的含硫化合物，这也是鲜洋葱味道更加辛辣、刺鼻的主要原因。 这里主要提到，新鲜的洋葱所表现出的味道，是由含硫氦基酸释放的，而含硫氨基酸，是由前体物质形成的。 细胞组织被破坏后,含硫氦基酸的前体物质(S-烷基-1-半胱氨酸亚砜、蒜氨酸等）被释放，在蒜氨酸酶的作用下形成。","categories":[{"name":"日常考据","slug":"日常考据","permalink":"https://dangyoo.github.io/categories/%E6%97%A5%E5%B8%B8%E8%80%83%E6%8D%AE/"}],"tags":[{"name":"烹饪","slug":"烹饪","permalink":"https://dangyoo.github.io/tags/%E7%83%B9%E9%A5%AA/"}]},{"title":"个人博客搭建攻略","slug":"个人博客搭建攻略","date":"2023-03-09T07:17:10.000Z","updated":"2024-03-31T08:40:03.212Z","comments":true,"path":"2023/03/09/个人博客搭建攻略/","link":"","permalink":"https://dangyoo.github.io/2023/03/09/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%94%BB%E7%95%A5/","excerpt":"闲言碎语最近心血来潮，想把自己的知识库迁移到一个统一的地方。 之前用过一段时间语雀，在线编辑实在是有点卡，用了一年放弃了。 后来改用 Notion，操作非常方便，但是页面多起来以后，时常卡顿。 另外这类应用都把内容存到了服务器上，说实话没有本地备份还是有些不放心的。 考虑到主要目的是将知识库做好分类，并且保存到一个看起来不太会丢的地方，不需要各种各样的多媒体内容，大多是文字的东西，好像建个站更方便一些。","text":"闲言碎语最近心血来潮，想把自己的知识库迁移到一个统一的地方。 之前用过一段时间语雀，在线编辑实在是有点卡，用了一年放弃了。 后来改用 Notion，操作非常方便，但是页面多起来以后，时常卡顿。 另外这类应用都把内容存到了服务器上，说实话没有本地备份还是有些不放心的。 考虑到主要目的是将知识库做好分类，并且保存到一个看起来不太会丢的地方，不需要各种各样的多媒体内容，大多是文字的东西，好像建个站更方便一些。 技术选型首先没有什么经验，先上知乎搜了下大概的技术类型，这篇文章讲的比较详细 静态博客技术选型最佳实践 根据需求，不考虑整域名和服务器，使用最快速的方法，毕竟内容更重要是吧，所以选定免费且用的人多所以教程丰富的 GitHub Pages + Hexo 配置和安装GitHub PagesGithub Pages 主要完成两个任务，一是存放内容，二是提供网页服务 Github Pages官方介绍 首先注册 github 账号，然后在主页 new 一个 repository，在 Repository name 一栏填写 username.github.io 这个username必须和你的账户名一致，如果账户名有大写字母，这里改为小写字母即可 底下的 Add a README file 最好选中，这样会直接创建一个主分支，如果没有文件的话仓库是没有分支的 创建好后进入对应 Repository，点击 Setting，侧边栏找到 Pages，设置好默认的分支 到这里 Github 上就已经操作好啦，这个 Repository 里存的就是你要对外呈现的内容，域名为 username.github.io Hexo由于 GitHub Pages 存放的都是静态文件，正常的博客存放的除了文章内容之外，还有文章列表、文章分类、文章标签等动态内容 这部分我们就可以用 Hexo 来完成 Hexo自称是一个快速、简洁且高效的博客框架，能够解析 Markdown 文章并生成静态页面 也就是能够解决文章列表、文章分类、文章标签等动态内容和文章页面的生成，我们只需要将文章内容提交到 GitHub 即可 Hexo官网 安装 Hexo 的前置任务是安装 Node.js 和 Git Node.js下载 Git下载 安装好 Node.js 后就可以使用 npm 来做 Hexo 的安装 npm install -g hexo 安装好后，在想要的根目录下运行以下命令进行初始化，我这里的根目录是 ./Blog hexo init 初始化后会生成一系列文件夹和文件，其中比较重要的是以下几个 _config.yml 配置文件，站点名字、主题、链接啥的都在这里改 themes 存储页面主题的文件夹，站点好看不好看就靠它 source 存储用户资源，比如文章内容、图片等 source&#x2F;_posts 存储文章内容 此时就可以对站点进行本地预览 hexo s 在浏览器中进入 http://localhost:4000/ 就能看到默认主题的网页啦 把 Hexo 和 Github Pages 结合起来修改 _config.yml 文件的内容，在文件最后找到 deploy，修改下面的内容，注意每一行英文分号后边的空格不要删掉 type: git repo: https://github.com/username/username.github.io branch: main repo 中的 username 需要替换为自己的账户名称 branch 和你在 GitHub Pages - Settings - Pages 中设置的分支一致 设置后在根目录执行命令 hexo clean hexo g &#x2F;&#x2F; 生成目录 hexo d &#x2F;&#x2F; 同步 Git 如果发生 ERROR Deployer not found: git 错误，则执行 npm install hexo-deployer-git –save 之后等一会儿就可以通过域名 username.github.io 在线访问博客了 常用操作新建一篇文章 hexo n “文章标题” 之后在任意编辑器中对文章进行编辑即可 美化Hexo 的页面美化是通过主题 theme 来实现的，在 _config.yml 文件可以看到一个 theme 项，默认值为 landscape，可以下载各种各样的主题来进行美化 比如本站用的就是 next 这个主题，另外常见的还有 butterfly 等主题 Hexo主题库 下载主题举例 butterfly 主题的下载和使用 首先在根目录下载主题到 themes 文件夹 git clone -b master https://gitee.com/immyw/hexo-theme-butterfly.git themes&#x2F;butterfly git clone https://github.com/theme-next/hexo-theme-next themes&#x2F;next git clone https://github.com/litten/hexo-theme-yilia.git themes&#x2F;yilia 然后修改 _config.yml 文件的 theme 值为 butterfly&#x2F;next&#x2F;yilia 即可 在文章中添加图片通过在 source 文件夹下新建存储图片的文件夹，如 images 然后将所需要的图片放在 images 文件夹下 在文章中使用 ![img](/images/a.jpg) 即可 如果需要调整图片大小，可以直接使用 HTML 的 img 标签 &lt;img src=&quot;/images/a.jpg&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt; 隐藏文章当一篇文章被设置为「隐藏」时，它不会出现在任何列表中（包括首页、存档、分类页面、标签页面、Feed、站点地图等），也不会被搜索引擎索引（前提是搜索引擎遵守 noindex 标签） npm install hexo-hide-posts –save 如果安装过程中出现 run npm audit fix to fix them, or npm audit for details 提示，尝试运行 npm audit fix npm set audit false 在 _config.yml 中添加如下设置 12345678910# hexo-hide-postshide_posts: # 可以改成其他你喜欢的名字 filter: hidden # 指定你想要传递隐藏文章的位置，比如让所有隐藏文章在存档页面可见 # 常见的位置有：index, tag, category, archive, sitemap, feed, etc. # 留空则默认全部隐藏 public_generators: [] # 为隐藏的文章添加 noindex meta 标签，阻止搜索引擎收录 noindex: true 在想要隐藏的文章中，添加 hidden 属性 12345---title: &#x27;Hidden Post&#x27;date: &#x27;2021/03/05 21:45:14&#x27;hidden: true--- 换台电脑继续建站在公司使用电脑A搭好以后，回家想用电脑B继续写文章，结果发现 hexo d 传到 Git 上的是已经解析好的静态网页文件，也就是只有 public 文件夹中的东西，其他属于 hexo 框架的部分都没有上传 所以需要把框架内容也同步到 Git 上，换电脑之后 pull 到本地继续编辑 在 Git 上另外建一个分支 hexo，与静态文件所在的分支分开，并将 hexo 分支设为 default 在本地原本根目录 ./Blog 之外的地方，重新 git clone 这个 hexo 分支，例如 hexo 分支目录为 ./hexo 清空 ./hexo 中的内容，把 ./Blog 目录中的内容复制到./hexo 中 运行 git add . git commit -m &quot;hexo框架&quot; git push，将内容更新到 hexo 分支 这样 Git 项目上，两个分支就分别为 hexo 框架内容 和 博客网页内容 了，以上的操作都发生在电脑A 换新电脑B之后，先安装 Node.js 和 git 然后 git clone hexo 分支，就可以直接进行博客编辑了，由于配置文件 _config.yml 中跟踪的分支是静态文件分支 main，所以使用 hexo d 提交时也会把内容提交到 main 分支 每次在电脑B或电脑A编辑之后，除了使用 hexo d 提交网页内容外，还要记得使用 git push 提交更新后的框架内容 使用遇到的问题 提示“无法加载文件hexo.ps1，因为在此系统上禁止运行脚本转载” 是Windows10下的策略设置问题，用管理员方式打开PowerShell（搜索PowerShell后选择管理员打开）并执行 set-ExecutionPolicy RemoteSigned 输入Y 即可 提示“Cannot find module ‘hexo’ from…” 安装包的问题，可能是误删了什么地方的东西，运行 npm install hexo –save 即可 hexo g后提示“NO LAYOUT” 确认主题是否还在themes文件夹下，可能被误删了 开启分类页面之后无法生成分类的首页 Cannot GET &#x2F;categories&#x2F; 首先要给分类页面建一个首页，运行 hexo new page “categories” 然后再source目录下找到新建的categories文件夹，找到里边的index.md文件，添加对应的type title: categories date: xxxx-xx-xx xx:xx:xx type: “categories” 即可 开启阅读统计 next主题下的配置文件中，设置busuanzi_count.enable: true即可 开启百度统计 在百度统计中添加网站首页信息，在next主题下的配置文件中，设置baidu_analytics: xxx（xxx为百度统计提供的hm.baidu.com&#x2F;hm?js后边的内容） 参考文章快速搭建个人博客 —— 保姆级教程 Hexo建站手册（详细教程） 有哪些好看的 Hexo 主题 Hexo 添加图片——不用插件，超简单 How to fix npm vulnerabilities manually NexT使用文档","categories":[{"name":"小项目","slug":"小项目","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/"},{"name":"博客搭建","slug":"小项目/博客搭建","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"实践","slug":"实践","permalink":"https://dangyoo.github.io/tags/%E5%AE%9E%E8%B7%B5/"}]},{"title":"Kafka","slug":"Kafka","date":"2022-09-15T10:06:03.000Z","updated":"2024-03-31T08:40:03.204Z","comments":true,"path":"2022/09/15/Kafka/","link":"","permalink":"https://dangyoo.github.io/2022/09/15/Kafka/","excerpt":"闲言碎语没啥好说的，Kafka算是每个公司都会用到的东西了吧，虽然知道是消息队列，但具体怎么队列的，和其他的比如RabbitMQ有啥区别，确实不太懂。","text":"闲言碎语没啥好说的，Kafka算是每个公司都会用到的东西了吧，虽然知道是消息队列，但具体怎么队列的，和其他的比如RabbitMQ有啥区别，确实不太懂。 进程间的通信方式消息队列（Message queue）是一种进程间通信或同一进程的不同线程间的通信方式，除了消息队列，进程间的通信方式（Inter-Process Communication，简称IPC）还有：无名管道（pipe）、高级管道（popen）、有名管道（named pipe）、信号量（semophore）、共享内存（shared memory）、套接字（socket） 无名管道管道是一种半双工的通信方式，数据只能同一时间单向流动，而且只能在具有亲缘关系的进程间使用，进程间的亲缘关系同化成那个指的是父子进程关系。 半双工半双工（Half Duplex）是指数据可以在一个信号载体的两个方向上传输，但是不能同时传输。全双工，即信息可以在两个方向上同时传输。单工则只能在一个方向传输。 高级管道讲一个程序当做一个新的进程在当前程序进程中启动，则它算是当前进程的紫禁城，这种方式称之为高级管道。 有名管道也是半双工通信方式，但允许无情缘关系的进程间通信。 消息队列消息队列是消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息量烧、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号量一个计数器，可以用来控制多个进程对共享资源的访问。常作为一个锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源导致冲突。 共享内存指的是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的进程间通信方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制如信号量配合使用，来实现进程间的同步和通信。 套接字可以用于不同机器间的进程通信。 消息队列Message Queue（MQ），屏蔽底层复杂的通讯协议，定义了一套应用层更加简单的通讯协议 一个分布式系统中的两个模块之间的通讯要么是HTTP，要么是自己开发的（rcp）TCP HTTP协议很难实现两端通讯，即A调用B同时B也可以调用A，想要实现的话AB都需要WebServer ，而实现TCP则更加复杂 MQ做的就是在这些协议至上构建一个更高层次的、更简单的生产者&#x2F;消费者通讯模型，它定义了两个对象，发送数据的生产者和接收数据的消费者，并提供一个SDK来定义而这实现消息通讯而无视底层通讯协议 一些相关概念HTTP协议Hyper Text Transfer Protocal，超文本传输协议，是一个基于TCP&#x2F;IP通信协议来传递数据的协议，它工作于客户端-服务端架构上，客户端通过URL向服务端（WebServer）发送请求，服务端接收到请求后，向客户端发送响应信息 HTTP是无连接，即限制每次连接只能处理一个请求，服务器处理完客户的请求并收到客户的应答后，即断开连接，采用这种方式可以节省传输时间 HTTP是媒体独立的，即只要客户端和服务器知道如何处理数据内容，任何类型的数据都可以通过HTTP发送，客户端及服务器指定使用合适的MIME-type内容类型 HTTP是无状态协议，指协议对于事务处理没有记忆能力，缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大，另一方面，在服务器不需要先前信息时它的应答就较快 MIMEMultipurpose Internet Main Extensions，描述消息内容类型的标准，用来标识文档、文件或字节流的性质和格式，浏览器通常使用MIME类型来确定如何处理URL，因此WebServer在响应头中添加正确的MIME类型非常重要，如果配置不正确，浏览器可能会无法解析文件内容，网站将无法正常工作，下载的文件也会被错误处理 MIME类型通用结构为type&#x2F;subtype，常见的有 text&#x2F;html -&gt; 超文本标记语言文本 text&#x2F;plain -&gt; 普通文本 imge&#x2F;gif -&gt; GIF图形 RCPRemote Copy，远程复制，是在Unix操作系统中用于在计算机之间远程复制一个或多个文件的命令，通过TCP&#x2F;IP协议传输，已经被更安全的协议和命令所渠道，如SCP（基于安全Shell的安全副本）和SFTP（简单文件传输协议） TCP&#x2F;IPTransmission Control Protocol&#x2F;Internet Protocol，传输控制协议&#x2F;网际协议，定义了电子设备如何连入因特网，以及数据如何在它们之间传输 TCP&#x2F;IP中包含一系列用于处理数据通信的协议，如TCP用于应用程序之间的通信，UDP（用户数据报协议）用于应用程序之间的简单通信，IP用于计算机之间的通信，ICMP（因特网消息控制协议）是针对错误和状态的通信，DHCP（动态主机配置协议）是针对动态寻址的协议 TCP负责将应用程序的数据分割并装入IP包，然后在到达的时候重新组合它们，而IP则负责将包发送给接受者 SDKSoftware Development Kit，软件开发工具包，即可用于开发面向特定平台软件应用程序的工具包，类似于Python中的TensorFlow包，就是谷歌提供的TensorFlow对应的SDK 消息队列的分类有Broker的MQ有服务器作为Broker，所有消息都通过它中转，生产者把消息发送给Broker就结束了自己的任务，Broker着把消息主动推送给消费者或等待消费者主动轮询 重TopicKafka、ActiveMQ（JMS）属于重Topic类型，生产者会发送key和数据到Broker，由Broker比较key之后决定给哪个消费者消费 在这种模式下，一个topc往往是一个较大的概念，甚至一个系统中可能只有一个topic，topic某种意义上就是queue 虽然架构一致，但Kafka的性能比ActiveMQ高很多，所以这种类型的MQ只有Kafka一种备选方案 RocketMQ是阿里基于Kafka重写的，保证消息必答而牺牲了性能，Kafka单机写入在百万条&#x2F;秒，而RocketMQ则在7万条&#x2F;秒，更适用于业务处理，而Kafka更适用于日志处理 轻TopicRabbitMQ（AMQP）属于轻Topic类型，生产者发送key和数据，消费者定义订阅的队列，Broker收到数据之后会通过一定的逻辑计算出key对应的队列，然后把数据交个队列 这种模式下解耦了key和queue，这种架构中queue是非常轻量级的，消费者只关心自己的queue，生产者不用关心数据最终给谁只要指定key就行，中间的映射层在AMQP中称为Exchange交换机 AMQP中有四种Exchange，分别是 Direct exchange：key等于queue Fanout exchange：无视key，给所有queue都来一份 Topic exchange：key可以模糊匹配queue Headers exchange：无视key，通过查看消息头部元数据来决定发给哪个queue 无Broker的MQZeroMQ被设计成了一个库，而非中间件，更加轻量和灵活 节点之间通讯的消息都发送到彼此的队列中，每个节点都既是生产者优势消费者，ZeroMQ做的就是封装出一套类似于Socket的API来完成发送、读取数据 消息队列的优势 解耦：允许你独立地扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束； 可恢复性：系统的一部分组件失效时，不会影响到整个系统； 缓冲：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致情况； 灵活性&amp;峰值处理能力：削峰，能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷请求而完全崩溃； 异步通信：提供异步处理机制，允许用户把消息放入队列，而不去处理，在想要去处理的时候再去处理。 消息队列的两种模式 点对点模式一对一，消费者主动拉取数据，消息收到后消息清除。消息生产者生产消息发送到队列中，然后消息消费者从队列中取出并且消费消息。消息被消费后，队列中不再有存储，所以消息消费者不可能消费到已经被消费的消息。 发布&#x2F;订阅模式一对多，消费者消费数据之后不会清除消息。 消息生产者将消息发布到topic（队列）中，同时有多个信息消费者（订阅）消费该消息，根据消费方式不同又分为两种： 消费者主动订阅（Kafka的模式），消费者的消费速度由消费者自己定义，但需要轮询topic中是否有新消息； 队列主动推送（RabbitMQ的模式）。 KafkaKafka最初由Linkedin公司开发，是一个分布式的、支持分区（partition）的、多副本（replica）的，基于Zookeeper协调的发布&#x2F;订阅模式的消息系统，它最大的特性就是可以实时处理大量数据以满足各种需求场景，如基于Hadoop的批处理系统、低延迟的实时系统、Storm&#x2F;Flink流式处理引擎、Web&#x2F;Nginx日志、访问日志、消息服务等，用Scala编写。 使用场景 日志收集：用Kafka来收集各种服务的Log，通过Kafka以统一接口服务的方式开放给各种消费者 消息系统：解耦生产者和消费者，缓存消息 用户活动跟踪：记录Web用户或者App用户的各种活动，如浏览网页、搜索、点击等，这些活动信息被各个服务器发布到Kafka的Topic中，然后消费者通过订阅这些Topic来做实时的监控分析，或者装载到Hadoop进行离线分析和挖掘 运营指标：Kafka也经常用来记录运营监控数据，包括收集各种分布式应用的数据，生产各种操作的集中反馈，例如报警或警告 搭建1234567891011121314# 下载包，基于Scala2.13构建的最新3.5.1版本wget https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgztar -zxvf kafka_2.13-3.5.1.tgzcd kafka_2.13-3.5.1/config/# 编辑服务端配置文件vim server.properties# listeners=PLAINTEXT://localhost:9092 # 服务器地址和监听端口# log.dirs=xxx 存储日志文件的地址# zookeeper.connet=localhost:2181 # 所需连接的Zookeeper端口# 后台启动cd ../bin./kafka-server-start.sh -daemon ../config/server.properties# 检查是否启动成功ps -aux | grep server.properties 基本架构 Broker一台Kafka服务器就是一个Broker，一个集群由多个Broker组成，每个Broker可以容纳多个Topic Producer消息生产者，将消息push到Kafka集群中的Broker 12# 启动一个Producer，向Topic-test发送消息./kafka-console-producer.sh --broker-list localhost:9092 --topic test Consumer消息消费者，从Kafka集群中pull消息，消费消息 12345678# 启动一个Consumer，消费Topic-test的消息# 从最后一条消息的偏移量+1开始消费./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test# 从头开始消费./kafka-console-consumer.sh \\--bootstrap-server localhost:9092 \\--from-beginning \\--topic test Topic消息的类别或者主题，逻辑上可以理解为队列。Producer只关注push消息到哪个Topic，Consumer只关注订阅了哪个Topic 123456789101112131415161718192021222324# 创建Topic-test，只有一个分区，备份也是一个./kafka-topics.sh --create \\--bootstrap-server localhost:9092 \\--replication-factor 1 \\--partitions 1 \\--topic test# Created topic test.# 列出所有Topic./kafka-topics.sh --list --bootstrap-server localhost:9092# test# 查看test主题的信息./kafka-topics.sh --describe --topic test --bootstrap-server localhost:9092# Topic: test TopicId: VjYQwUnxSquIDuoHR3HbsQ PartitionCount: 1 ReplicationFactor: 1 Configs: # Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0# 3.0以下的版本，Topic交给Zookeeper管理# 创建Topic-test，只有一个分区，备份也是一个./kafka-topics.sh --create --zookeeper localhost:2181 \\--replication-factor 1 \\--partitions 1 \\--topic test# 列出所有Topic./kafka-topics.sh --list --zookeeper localhost:2181 Consumer Group消费者组，由一到多个Consumer组成，每个Consumer都属于一个Consumer Group，消费者组在逻辑上是一个订阅者。 消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费，消费者组之间互不影响。即每条消息只能被Consumer Group中的一个Consumer消费，但是可以被多个Consumer Group组消费，这样就实现了单播和多播。 消费者组的存在提高了同一个Topic的消费能力 12345678910111213141516# 创建一个消费者组中的消费者./kafka-console-consumer.sh --bootstrap-server localhost:9092 \\--consumer-property group.id=testGroup \\--topic test# 查看消费者组的信息./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list# testGroup# 查看某个消费者组的信息./kafka-consumer-groups.sh --bootstrap-server localhost:9092 \\--describe --group testGroup# GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID# testGroup test 0 5 5 0 console-consumer-fe47d017-2097-439e-9534-f0bf4b409345 /127.0.0.1 console-consumer# # CURRENT-OFFSET：该TOPIC被该消费者组消费的最新OFFSET# LOG-END-OFFSET：该TOPIC最新消息OFFSET# LAG：LOG-END-OFFSET - CURRENT-OFFSET：多少消息没被消费 单播 - 同一条消息只能被同一个消费者组中的一个消费者消费 多播 - 同一条消息能被不同消费者组中的不同消费者消费 Partition负载均衡与扩展性考虑，一个Topic可以分为多个Partition，物理存储在Kafka集群中的多个Broker上。可靠性上考虑，每个Partition都会有备份Replica Kafka只能在Partition范围内保证消息的局部顺序性，而不能在同一个Topic中的多个Partition中保证总的消费顺序性 Consumer会定期将自己消费分区的offset提交给Kafka内置的topic：__consumer_offsets，提交过去的时候，key是Consumer Group+topic+分区号，value是当前的offset值，Kafka会定期清理topic里的消息，最后就保留最新的那条数据 因为__consumer_offsets可能会接收高并发的请求，Kafka默认分配50个分区（可以通过offsets.topic.num.partition设置） 通过公式（ hash(consumerGroupId)%__consumer_offsets的分区数 ）可以计算Consumer消费的offset要提交到__consumer_offsets的哪个分区 123456789./kafka-topics.sh --create --bootstrap-server localhost:9092 \\--replication-factor 1 \\--partitions 2 \\ # 声明多个分区--topic test1# 创建的保存Log的文件夹名为test1-0\\test1-1# .index存储的是数据offset索引# .timeindex存储的是时间索引# .log存储的是具体数据 ReplicaPartition的副本，为了保证集群中的某个节点发生故障时，该节点上的Partition数据不会丢失，且Kafka仍能继续工作，所以Kafka提供了副本机制，一个Topic的每个Partition都有若干个副本，一个Leader和若干个Follower 123456789./kafka-topics.sh --create --bootstrap-server localhost:9092 \\--replication-factor 3 \\ # 声明3个副本--partitions 2 \\ # 声明多个分区--topic test2# ./kafka-topics.sh --describe --topic test2 --bootstrap-server localhost:9092# Topic：test2 partitionCount: 2 ReplicationFactor: 3 configs:# Topic: test2 Partition: 0 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1# Topic: test2 Partition: 1 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2 LeaderReplica的主角色，Producer与Consumer只跟Leader交互 FollowerReplica的从角色，实时从Leader中同步数据，保持和Leader数据的同步。Leader发生故障时，某个Follower会变成新的Leader ControllerKafka集群中的其中一台服务器，用来进行Leader election以及各种Failover（故障转移），每个Broker启动时会向Zookeeper创建一个临时序号节点，序号最小的节点将作为集群的Controller 当某个分区的Leader副本出现故障时，由Controller负责为该分区选举新的Leader 当检测到某个分区的ISR集合发生变化时，由Controller负责通知所有的Broker更新其元数据信息 当使用Kafka-topics.sh脚本为某个Topic增加分区数量时，由Controller负责让新分区被其他节点感知到 ZookeeperKafka通过Zookeeper存储集群的meta等信息（0.9版本之后消费者的offset信息粗处在Kafka系统中） 1234567891011# 进入Zookeeper客户端下./zkCli.shls /# [admin, brokers, cluster, config, consumers, # controller, controller_epoch, feature, # isr_change_notification, latest_producer_id_block, log_dir_event_notification]# 说明已经成功连接Zookeeper服务ls /brokers# [ids, seqid, topics]ls /brokers/ids# [0] 搭建集群修改server.properties中的配置项 broker.id，listeners，log.dir 启动所有节点 配置ACKKafka服务端接收到Producer消息之后，返回已接收到的消息元数据 配置ack &#x3D; 0时，Producer只要把消息发送出去，Kafka就会返回ACK 配置ack &#x3D; 1时，Producer发送消息到Leader之后，Leader把消息写入到本地文件中，然后返回ACK 配置ack &#x3D; -1或all时，Producer发送消息到Leader之后，Leader把消息写入到本地文件，且数据被同步到（min.insync.replicas&#x3D;1）台Follower中后，才会返回ACK BatchProducer处会生成一个缓冲区（Buffer Memory 默认32M），消息先进入缓冲区中，另外存在一个本地线程去缓冲区中拉取一定大小（Batch Size 默认16K）的数据，缓冲区数据小于16K时，每隔一定时间（Linger ms 默认10ms）拉取剩余数据，发送到Kafka中 OffsetConsumer消费消息时，会去Kafka服务端的Topic中拉取（poll）一定长度的消息到Consumer所在服务器，如果设置为自动提交offset，则在消息拉取回来后，不管消息是否已经被消费，就会将offset提交到Kafka服务端，也就是说，自动提交是有可能造成消息丢失的 另一种设置是手动提交offset，指的是Consumer消费完信息后再进行提交，细节又分为两种 手动同步提交：消费线程在offset提交前会一直阻塞 手动异步提交：消费线程不会因为offset提交未完成而阻塞 核心机制ISRIn-Sync Replicas，指所有与Leader保持一定程度（可配置时间&#x2F;准确度）同步的副本（包括Leader自己），Leader负责维护和跟踪ISR集合中所有Follower的之后状态，当某个Follower滞后太多或失效时，Leader会把它从ISR集合中剔除，当Leader失效时，只有在ISR集合中的副本才有资格被选举为新的Leader Rebalance前提是消费者没有指明分区消费，此时当消费者组里的消费者和分区关系发生变化时，就会触发Rebalance机制，来重新调整消费者和分区的关系 Range：通过公式来计算哪个消费者消费哪个分区 轮询：轮流消费 Sticky：先确保原有消费关系不变，再进行调整 HW和LEOHigh Watermark，高水位，取一个Partition对应的ISR中最小的LEO（LOG-END-OFFSET）作为HW，Consumer最多只能消费到HW所在的位置。另外每个Replica都有HW，Leader和Follower各自负责更新自己的HW状态。对于Leader新写入的消息，Consumer不能立刻消费，Leader会等待该消息被所有ISR中的Replica同步后更新HW，之后Consumer才能进行消费。这样就保证了如果Leader所在的Broker失效，消息仍然能从新选举出的Leader中获得 JAVA使用Kafka引入依赖12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;Kafka&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;!--&gt;在SpringBoot中的依赖为&lt;--&gt; &lt;!--&gt;&lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;&lt;--&gt; &lt;!--&gt;&lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;&lt;--&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt; &lt;version&gt;1.7.30&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;!--&gt;这个包用来解决Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;报错&lt;--&gt; &lt;/dependencies&gt;&lt;/project&gt;&lt;!--&gt;import org.apache.kafka.clients&lt;--&gt; Producer12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package cn.dy;import org.apache.kafka.clients.producer.*;import org.apache.kafka.common.serialization.StringSerializer;import java.util.Properties;import java.util.concurrent.ExecutionException;/** * 创建一个Kafka的生产者类 */public class MyProducer &#123; private final static String TOPIC_NAME = &quot;my-topic&quot;; // 声明要生产的主题名称 public static void main(String[] args) throws ExecutionException, InterruptedException &#123; // 设置参数 Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;43.153.211.186:9092&quot;); // 把发送的Key从字符串序列化为字节数组 props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // 把发送消息的Value从字符串序列化为字节数组 props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // 创建生产者，传入参数 Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); // 创建消息，Key决定了发往哪个分区，Value是具体发送的消息内容 ProducerRecord&lt;String, String&gt; producerRecord = new ProducerRecord&lt;&gt;(TOPIC_NAME, &quot;myKey&quot;, &quot;HelloWallet&quot;); // 同步发送消息，获取消息元数据并输出 try &#123; RecordMetadata recordMetadata = producer.send(producerRecord).get(); // 使用get方法获取服务端返回的ACK，如果没有返回则会阻塞 System.out.println(&quot;同步发送结果：&quot; + &quot;topic - &quot; + recordMetadata.topic() + &quot; partition - &quot; + recordMetadata.partition() + &quot; offset - &quot; + recordMetadata.offset()); &#125; catch (InterruptedException e)&#123; // 用try-catch驳货异常 e.printStackTrace(); Thread.sleep(1000); // 重试间隔 try &#123; RecordMetadata recordMetadata = producer.send(producerRecord).get(); &#125; catch (Exception e1) &#123; // 重试失败，进行其他处理 &#125; &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; // 异步发送消息，异步容易产生消息丢失，真正使用时同步用得更多 producer.send(producerRecord, new Callback() &#123; @Override public void onCompletion(RecordMetadata recordMetadata, Exception e) &#123; if (e != null) &#123; System.out.println(&quot;消息发送失败：&quot; + e.getStackTrace()); &#125; if (recordMetadata != null) &#123; System.out.println(&quot;异步发送结果：&quot; + &quot;topic - &quot; + recordMetadata.topic() + &quot; partition - &quot; + recordMetadata.partition() + &quot; offset - &quot; + recordMetadata.offset()); &#125; &#125; &#125;); Thread.sleep(10000L); // 主线程关闭不会打印出来，所以延迟关闭 &#125;&#125;/* 执行后报错could not be established. Broker may not be available.使用命令sudo lsof -i:9092可以看到localhost:9092 (LISTEN)先检查防火墙是否开启了9092端口然后修改server.properties中的listeners=PLAINTEXT://0.0.0.0:9092表示监听所有IP的访问此时重启Kafka使用命令sudo lsof -i:9092可以看到*:9092 (LISTEN)*/ Consumer1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package cn.dy;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.serialization.StringDeserializer;import java.time.Duration;import java.util.Arrays;import java.util.Properties;public class MyConsumer &#123; private static final String TOPIC_NAME = &quot;my-topic&quot;; // 想要消费的Topic名称 private static final String CONSUMER_GROUP_NAME = &quot;testGroup&quot;; // 所属的消费者组名称 public static void main(String[] args) &#123; Properties props = new Properties(); // 配置信息 props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;43.153.211.186:9092&quot;); // 服务IP props.put(ConsumerConfig.GROUP_ID_CONFIG, CONSUMER_GROUP_NAME); // 消费者组 //props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &quot;false&quot;); // 是否自动提交offset /*默认设置为true，也可以声明自动提交offset的时间间隔 * props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, &quot;true&quot;); * props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, &quot;1000&quot;);*/ props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, &quot;earliest&quot;); /*当消费主体的是一个新消费者组，或者指定offset的消费方式 * latest：只消费自己启动之后发送到主题的消息 * earliest：首次启动时从头开始消费，下次启动根据offset继续消费*/ //consumer.seekToBeginning 配置则会每次都从头开始消费 //props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 1000); // Consumer给broker发送心跳的间隔时间 //props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 10*1000); /*Kafka如果超过这个时间没有收到消费者的心跳，就会把消费者踢出消费者组，进行rebalance，将分区分配给其他消费者*/ //props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500); // 一次poll最大拉取消息的条数，根据消费速度来设置 //props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 30*1000); // 两次poll间隔超过这个事件，Kafka认为消费能力过弱，将其踢出消费者组 // 把收到的Key从字节数组转化为字符串 props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); // 把发送消息的Value从字节数组转化为字符串 props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); // 创建消费者客户端 consumer.subscribe(Arrays.asList(TOPIC_NAME)); // 订阅主题列表 while (true) &#123; ConsumerRecords&lt;String, String&gt; consumerRecords = consumer.poll(Duration.ofMillis(1000)); // 拉取消息的长轮询 for (ConsumerRecord&lt;String, String&gt; consumerRecord: consumerRecords) &#123; // 打印消息 System.out.printf(&quot;收到消息：partition = %d， offset = %d， key = %s， value = %s%n&quot;, consumerRecord.partition(), consumerRecord.offset(), consumerRecord.key(), consumerRecord.value()); &#125; &#125; &#125;&#125; 线上问题优化防止消息丢失Producer：ack设置为-1或all，确保足够多的副本都完成消息同步再返回ACK Consumer：将自动提交改为手动提交，确保消息被消费后再更新offset 防止消息重复消费Producer：当Producer发送消息后，由于网络问题未收到Kafka服务端返回的ACK，则会进行重试，此时服务端就会造成消息重复，如果从Producer端考虑防止，则应该关闭重试机制，但为了确保消息不丢失，又不应该关闭重试，所以应该考虑在Consumer端进行解决 Consumer：当Consumer收到消息后，需要一个机制来确保当前消息与之前已经消费过的消息不重复，可以通过幂等性保证来解决，例如通过主键来判断重复性或使用Redis&#x2F;Zookeeper的分布式锁（主流方案） 幂等性在数学中某一元运算为幂等时，其作用在任一元素两次后会和其作用一次的结果相同。在软件工程中，指的是函数&#x2F;接口可以使用相同的参数重复执行，不影响系统的状态也不会对系统造成改变 幂等性保证需要满足三个条件： 请求唯一标识，即每一个请求必须有一个唯一标识； 处理唯一标识，即每次处理完请求之后，必须有一个记录标识这个请求被处理过了 逻辑判断处理，即每次接收请求需要进行判断之前是否处理过，也就是根据请求唯一标识查询是否存在处理唯一标识 幂等性常见的实现方案有： token机制：针对客户端重复连续多次点击的情况，提交接口需要通过token机制实现防止重复提交，主要流程为：a. 服务端提供生成请求token的接口，在存在幂等问题的业务执行前，向服务器请求获取token，服务器会把token保存到Redis中b. 调用业务接口请求时，把token携带过去，一般放在请求头中c. 服务器判断请求token是否存在于Redis中，存在则表示为第一次请求，这时把Redis中的token删除，继续执行业务；如果判断token不存在于Redis，则表示是重复操作，直接返回重复标记给客户端 数据库索引唯一：往数据库表里插入数据的时候，利用数据库的唯一索引特性来保证唯一性 Redis实现：将唯一序列号作为key存入Redis，在请求处理前先查看key是否存在，不存在则表示为处理过，存在则表示已经处理过 状态机：在业务处理中，通过业务流转状态控制请求的幂等 分布式锁：一篇文章 实现顺序消费Producer：ack不能设置为0，关闭重试，使用同步发送，确保消息是顺序发送的 Topic：设置为单一分区，因为Kafka只确保单一分区中的消息有序 Consumer：由于是单一分区，此时同一个消费者组里只能有一个消费者，这个消费者消费到的消息是顺序的 解决消息积压Producer的生产速度超过Consumer消费速度，会导致消息积压，进而引发Kafka性能问题或服务崩溃，一般由以下方案： 在一个消费者中启动多个线程进行消费，提升单一消费者的消费能力； 在一个或多个服务器上启动多个消费者，提高消费能力； 让一个消费者将积压Topic中的消息发送到另一个新的Topic中，新Topic设置多个分区和多个消费者 实现延迟队列业务场景：如果订单创建30分钟内没有付款，则需要取消订单。这个业务场景可以通过延时队列来实现 创建多个Topic，每个Topic表示延时的间隔 topic_5s：延时5s后需要执行操作的消费者需要消费的主题 topic_1m：延时1min后需要执行操作的消费者需要消费的主题 topic_30m：延时30min后需要执行操作的消费者需要消费的主题 Producer将消息发送到相应的Topic中，并带上消息的发送时间 消费者订阅相应的Topic，消费时轮询Topic中的消息 如果消息的发送时间和当前消费时间之差超过预设值，则进行某些操作（如超过30min，则去数据库判断订单是否已付款，如未付款则标注这个订单已取消） 如果未超过预设值，则不消费当前offset及之后的消息 等待一定的时间之后，继续从上次消费截止的offset开始poll消息进行判断 KafkaEagle监控平台1234567891011121314151617181920212223242526272829303132333435363738394041# 下载wget https://github.com/smartloli/kafka-eagle-bin/archive/v3.0.1.tar.gz# 解压tar -zvxf v3.0.1.tar.gzcd kafka-eagle-bin-3.0.1/tar -zxvf efak-web-3.0.1-bin.tar.gz mv efak-web-3.0.1 ../../kafka-eagle-web# 配置vim /etc/profile# 增加KE_HOME和PATH配置# export KE_HOME=../kafka-eagle-web# export PATH=$PATH:$KE_HOME/bin# ke配置文件修改vim conf/system-config.properties# Zookeeper相关配置# efak.zk.cluster.alias=cluster1# cluster1.zk.list=xxx.xxx.xxx.xxx:2181# 数据库相关配置，配置为MySQL，在MySQL中创建对应的库# 主要用来保存Eagle的元数据# efak.driver=com.mysql.cj.jdbc.Driver# efak.url=jdbc:mysql://...# efak.username# efak.password# 启动./ke.sh start# 启动时报错The KE_HOME environment variable is not defined correctly.# 需要重新加载系统配置# source /etc/profile# 再次启动报错The JAVA_HOME environment variable is not defined correctly.# 需要在/etc/profile里配置JAVA_HOME# 查询JAVA安装路径# which java# /usr/bin/java# ls -lrt /usr/bin/java# /usr/bin/java -&gt; /etc/alternatives/java# ls -lrt /etc/alternatives/java# /etc/alternatives/java -&gt; /usr/lib/jvm/java-11-openjdk-amd64/bin/java# 设置JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 应用场景Flume对接Kafka生产环境中，更多的是将数据生产到日志之中，从日志中获取数据，一般使用Flume。如果有多个消费者需要这个数据，则需要一个能够支持动态添加的中间件，也就是Kafka。使用Flume+Kafka，就可以实现一次采集，给多个消费者消费。 Flume不支持副本事件。于是，如果Flume代理的一个节点奔溃了，即使使用了可靠的文件管道方式，你也将丢失这些事件直到你恢复这些磁盘。如果你需要一个高可靠行的管道，那么使用Kafka是个更好的选择。 如果只是想把日志数据存储到HDFS，则可以Flume直接对接HDFS。 Kafka和RabbitMQ的差异 RabbitMQ是一个分布式消息代理，从多个来源收集流式处理数据，然后将其路由到不同的目标进行处理。而Kafka是一个流式处理平台，用于构建实时数据管道和流失处理应用程序，功能要比RabbitMQ复杂； RabbitMQ中生产者发送并监控消息是否达到目标消费者。而对于Kafka来说，无论消费者是否检索消息，生产者都会向队列发布消息； RabbitMQ允许生产者使用优先队列升级某些消息，即不按照先入先出的顺序发送，而是按照优先级来处理消息。而Kafka平等对待这些消息； RabbitMQ中生产者收到消费者发送的ACK后会将消息从队列中删除。而Kafka数据会一直保存到硬盘直到保留期限到期； 在消息传输容量方面，Kafka优于RabbitMQ。","categories":[{"name":"Data","slug":"Data","permalink":"https://dangyoo.github.io/categories/Data/"},{"name":"Integration","slug":"Data/Integration","permalink":"https://dangyoo.github.io/categories/Data/Integration/"},{"name":"Kafka","slug":"Data/Integration/Kafka","permalink":"https://dangyoo.github.io/categories/Data/Integration/Kafka/"}],"tags":[{"name":"知识","slug":"知识","permalink":"https://dangyoo.github.io/tags/%E7%9F%A5%E8%AF%86/"}]},{"title":"Flume","slug":"Flume","date":"2022-09-05T13:06:03.000Z","updated":"2024-03-31T08:32:16.313Z","comments":true,"path":"2022/09/05/Flume/","link":"","permalink":"https://dangyoo.github.io/2022/09/05/Flume/","excerpt":"闲言碎语想要搞微服务日志的解析，先要把日志搞到阿里云的SLS日志系统里，但是SLS又不掌控在自己手里，所以考虑调研一下开源的解决方案。","text":"闲言碎语想要搞微服务日志的解析，先要把日志搞到阿里云的SLS日志系统里，但是SLS又不掌控在自己手里，所以考虑调研一下开源的解决方案。 FlumeFlume是一个分布式，可靠且可用的系统，用于有效地收集，聚合大量日志数据并将其从许多不同的源移动到集中式数据存储中。Apache Flume的使用不仅限于日志数据聚合。由于数据源是可定制的，因此Flume可用于传输大量事件数据，包括但不限于网络流量数据，社交媒体生成的数据，电子邮件消息以及几乎所有可能的数据源。 特点可靠性当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为： end-to-end：收到数据Agent首先将Event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送 Store on failure：这也是Scribe采用的策略，当数据接收方Crash时，将数据写到本地，待恢复后，继续发送 Best effort：数据发送到接收方后，不会进行确认 可恢复性事件在通道中上演，该通道管理从故障中恢复。Flume支持持久的文件通道，该通道由本地文件系统支持。还有一个内存通道可以将事件简单地存储在内存队列中，这样速度更快，但是当代理进程死亡时，仍保留在内存通道中的任何事件都无法恢复 可扩展性Flume采用了三层架构，分别为Agent，Collector和Storage，每一层均可以水平扩展。其中，所有Agent和Collector由Master统一管理，这使得系统容易监控和维护，且Master允许有多个（使用ZooKeeper进行管理和负载均衡），这就避免了单点故障问题 可管理性 所有Agent和Collector由Master统一管理，这使得系统便于维护 多Master情况，Flume利用ZooKeeper和Gossip，保证动态配置数据的一致性 用户可以在Master上查看各个数据源或者数据流执行情况，且可以对各个数据源配置和动态加载 Flume提供了web 和shell script command两种形式对数据流进行管理 功能可扩展性 用户可以根据需要添加自己的Agent，Collector或者Storage Flume自带了很多组件，包括各种Agent（File，Syslog等），Collector和Storage（File，HDFS等） 架构 Web Server：数据产生的源头 Agent：Flume的核心就是Agent。Agent是一个Java进程，包含组件Source、Channel、Sink，且运行在日志收集端，通过Agent接收日志，然后暂存起来，再发送到目的地。（Agent使用JVM 运行Flume。每台机器运行多个Agent，但是在一个Agent中只能包含一个Source） Source：Agent核心组件之一，Source（源）用于从Web Server收集数据，然后发送到Channel（通道）。 Channel：Agent核心组件之一，Channel（通道）可以用来从Source接收数据，然后发送到Sink，Channel存放临时数据，有点类似队列一样。 Sink：Agent核心组件之一，Sink（接收器）用来把数据发送的目标地点，如上图放到HDFS中。 Event：整个数据传输过程中，流动的对象都是实现了org.apache.flume.Event接口的对象。Event也是事务保证的级别。 Flow：Event从源点到达目的点的迁移的抽象 AgentFlume 运行的核心是 Agent。Flume以Agent为最小的独立运行单位。一个Agent就是一个JVM。它是一个完整的数据收集工具，含有三个核心组件，分别是Source、Channel、Sink。通过这些组件， Event 可以从一个地方流向另一个地方。 SourceSource是数据的收集端，负责将数据捕获后进行特殊的格式化，将数据封装到事件（Event） 里，然后将事件推入Channel中。 Flume提供了很多内置的Source， 支持 Avro，Log4j，Syslog 和 HTTP POST（body为json格式）。可以让应用程序同已有的Source直接打交道，如AvroSource，SyslogTcpSource。 如果内置的Source无法满足需要， Flume还支持自定义Source。 ChannelChannel是连接Source和Sink的组件，可以将它看做一个数据的缓冲区（数据队列），它可以将事件暂存到内存中也可以持久化到本地磁盘上，直到Sink处理完该事件。 SinkSink从Channel中取出事件，然后将数据发到别处，可以向文件系统、数据库、Hadoop存数据，也可以是其他Agent的Source。在日志数据较少时，可以将数据存储在文件系统中，并且设定一定的时间间隔保存数据。 应用拦截器当我们需要对数据进行过滤时，除了我们在Source、Channel和Sink进行代码修改之外， Flume为我们提供了拦截器，拦截器也是Chain形式的。拦截器的位置在Source和Channel之间，当我们为Source指定拦截器后，我们在拦截器中会得到Event，根据需求我们可以对event进行保留还是抛弃，抛弃的数据不会进入Channel中。 数据流Flume的核心是把数据从数据源收集过来，再送到目的地。为了保证输送一定成功，在送到目的地之前，会先缓存数据，待数据真正到达目的地后，删除自己缓存的数据。Flume传输的数据的基本单位是Event，如果是文本文件，通常是一行记录，这也是事务的基本单位。Event从Source，流向Channel，再到Sink，本身为一个byte 数组，并可携带headers信息。Event代表着一个数据流的最小完整单元，从外部数据源来，向外部的目的地去。值得注意的是，Flume提供了大量内置的Source、Channel和Sink类型。不同类型的Source，Channel和Sink可以自由组合。组合方式基于用户设置的配置文件，非常灵活。比如：Channel可以把事件暂存在内存里，也可以持久化到本地硬盘上。Sink可以把日志写入HDFS，HBase，甚至是另外一个Source等等。Flume支持用户建立多级流，也就是说，多个Agent可以协同工作，并且支持Fan-in、Fan-out、Contextual Routing、Backup Routes，这也正是Flume强大之处。如图： 多个Agent顺序连接可以将多个Agent顺序连接起来，将最初的数据源经过收集，存储到最终的存储系统中。这是最简单的情况，一般情况下，应该控制这种顺序连接的Agent 的数量，因为数据流经的路径变长了，如果不考虑Failover的话，出现故障将影响整个Flow上的Agent收集服务。 这个例子里面为了能让数据流在多个Agent之间传输，前一个Agent的Sink必须和后一个Agent的Source都需要设置为AVRO类型并且指向相同的HostName（或者IP）和端口。 多个Agent复杂流这种情况应用的场景比较多，比如要收集Web网站的用户行为日志， Web网站为了可用性使用的负载集群模式，每个节点都产生用户行为日志，可以为每个节点都配置一个Agent来单独收集日志数据，然后多个Agent将数据最终汇聚到一个用来存储数据存储系统，如HDFS上。 可以通过使用 Avro Sink 配置多个第一层 Agent（Agent1、Agent2、Agent3），所有第一层Agent的Sink都指向下一级同一个Agent（Agent4）的Avro Source上（同样你也可以使用 thrift 协议的Source和Sink来代替）。Agent4上的Source将Event合并到一个Channel中，该Channel中的Event最终由HDFS Sink消费发送到最终目的地。 多路复用流Flume支持多路复用数据流到一个或多个目的地。这是通过使用一个流的多路复用器（multiplexer）来实现的，它可以复制或者选择数据流到一个或多个Channel上。很容易理解，复制就是每个Channel的数据都是完全一样的，每一个Channel上都有完整的数据流集合。 选择就是通过自定义一个分配机制，把数据流拆分到多个Channel上。 上图的例子展示了从Agent foo扇出流到多个Channel中。这种扇出的机制可以是复制或者选择。当配置为复制的时候，每个Event都被发送到3个Channel上。当配置为选择的时候，当Event的某个属性与配置的值相匹配时会被发送到对应的Channel。例如Event的属性txnType是Customer时，Event被发送到Channel1和Channel3，如果txnType的值是Vendor时，Event被发送到Channel2，其他值一律发送到Channel3，这种规则是可以通过配置来实现的。","categories":[{"name":"Data","slug":"Data","permalink":"https://dangyoo.github.io/categories/Data/"},{"name":"Integration","slug":"Data/Integration","permalink":"https://dangyoo.github.io/categories/Data/Integration/"},{"name":"Flume","slug":"Data/Integration/Flume","permalink":"https://dangyoo.github.io/categories/Data/Integration/Flume/"}],"tags":[{"name":"知识","slug":"知识","permalink":"https://dangyoo.github.io/tags/%E7%9F%A5%E8%AF%86/"}]},{"title":"CDC","slug":"CDC","date":"2022-08-25T09:06:03.000Z","updated":"2024-03-31T08:40:03.191Z","comments":true,"path":"2022/08/25/CDC/","link":"","permalink":"https://dangyoo.github.io/2022/08/25/CDC/","excerpt":"闲言碎语在上家公司（代称N司）时，遇到一个情况：由于传统行业对数据资产的重视程度不够（也可能是安全意识较强），所有开发在数据库进行删除操作时，都是用的真删除，而非互联网公司常见的伪删除。这就造成，数据仓库每天同步全量数据的话，可能会发生主键丢失的情况，同时由于数据库设计以及不同系统间数据不通等各种问题，下游使用数据时就会出现准确性被质疑的情况。 在伪删除方案无法推动的情况下，数据侧考虑获取业务数据库的修改日志，来补齐这部分被删除的信息，所以就去研究了一下常见的方案。","text":"闲言碎语在上家公司（代称N司）时，遇到一个情况：由于传统行业对数据资产的重视程度不够（也可能是安全意识较强），所有开发在数据库进行删除操作时，都是用的真删除，而非互联网公司常见的伪删除。这就造成，数据仓库每天同步全量数据的话，可能会发生主键丢失的情况，同时由于数据库设计以及不同系统间数据不通等各种问题，下游使用数据时就会出现准确性被质疑的情况。 在伪删除方案无法推动的情况下，数据侧考虑获取业务数据库的修改日志，来补齐这部分被删除的信息，所以就去研究了一下常见的方案。 啥叫CDCCDC全称是Change Data Capture，即变更数据捕获，它是数据库领域常见的技术，主要用于捕获数据库的一些变更，然后把变更数据发送到下游。它的应用比较广，可以做一些数据同步、数据分发和数据采集，还可以做ETL。 CDC的类型业界主要分为两种： 基于查询，客户端会通过SQL方式查询源库表变更数据，然后对外发送。这类技术是入侵式的，需要在数据源执行SQL语句，使用这种技术实现CDC会影响数据源的性能，通常需要扫描包含大量记录的整个表。常见工具有Sqoop&#x2F;DataX&#x2F;Kafka JDBC Source。 基于日志，这也是业界广泛使用的一种方式，一般是通过binlog方式。变更的记录会写入binlog，解析binlog后会写入消息系统，或直接基于Flink CDC进行处理。这种技术是非入侵性的，不需要在数据源执行SQL语句，通过读取源数据库的日志文件以识别对源库表的创建&#x2F;修改或删除数据。常见工具有Debezium&#x2F;Canal&#x2F;Maxwell。 DataXDataX是阿里巴巴开源的一个异构数据源离线同步工具，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS等各种异构数据源之间高效的数据同步功能。 异构数据源离线同步指的是将源端数据同步到目的端，但是端与端的数据源类型种类繁多，在没有DataX之前，端与端的链路将组成一个复杂的网状结构，非常零散无法把同步核心逻辑抽象出来。 为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。 所以，当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，就可以跟已有的数据源做到无缝数据同步。 DataX本身作为离线数据同步框架，采用Framework+plugin架构构建。将数据源读取和写入抽象成为Reader&#x2F;Writer插件，纳入到整个同步框架中。 Reader：数据采集模块，负责采集数据源的数据，将数据发送给Framework。 Writer：数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。 Framework：它用于连接Reader和Writer，作为两者的数据传输通道，并处理缓冲、并发、数据转换等问题。 核心模块DataX完成单个数据同步的作业，我们把它称之为Job，DataX接收到一个Job之后，将启动一个进程来完成整个作业同步过程。 DataX Job启动后，会根据不同的源端切分策略，将Job切分成多个小的Task(子任务)，以便于并发执行。 切分多个Task之后，DataX Job会调用Scheduler模块，根据配置的并发数据量，将拆分成的Task重新组合，组装成TaskGroup（任务组）。每一个TaskGroup负责以一定的并发运行完毕分配好的所有Task，默认单个任务组的并发数量为5。 每一个Task都由TaskGroup负责启动，Task启动后，会固定启动Reader-&gt;Channel-&gt;Writer的线程来完成任务同步工作。 DataX作业运行完成之后，Job监控并等待多个TaskGroup模块任务完成，等待所有TaskGroup任务完成后Job成功退出。否则，异常退出。 调度流程举例来说，用户提交了一个DataX作业，并且配置了20个并发，目的是将一个100张分表的MySQL数据同步到ODPS里面。 DataX的调度决策思路是： DataX Job根据分库分表切分成了100个Task。 根据20个并发，DataX计算共需要分配4个TaskGroup。 4个TaskGroup平分切分好的100个Task，每一个TaskGroup负责以5个并发共计运行25个Task。 优化常用的优化参数有：Channel（通道）&#x2F;SplitPk（切片）&#x2F;BatchSize（批数据大小） Channel - 通道，并发量，该设置对传输效率影响较为明显，设置为1时，即没有并发，此时同步速度均为8.9M&#x2F;s，将该设置调高之后，速率明显倍增，但增大到一定程度后，瓶颈就转到其他配置了。 SplitPk - 切片，MySQLReader进行数据抽取时，如果制定SplitPk，表示用户希望使用SplitPk代表的字段进行数据分片，DataX因此会启动并发任务进行数据同步，这样可以大大提高数据同步的效能。推荐SplitPk使用表主键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。目前SplitPk仅支持整型数据切分，不支持浮点&#x2F;字符串&#x2F;日期等其他类型。如果用户指定其他非支持类型，MySQLReader将报错。如果SplitPk不填写，DataX视作使用单通道同步该表数据，并发需要与Channel设置配合。 BatchSize - 批数据大小，一次性批量提交的记录数大小，该值可以极大减少DataX与MySQL的网络交互次数，并提升整体吞吐量，默认为1024MB，过大可能会造成DataX运行进程OOM。 优缺点Datax的优势非常明显： 首先，部署非常简单，无论是在物理机上或者虚拟机上，只要网络通畅，便可进行数据同步，给实施人员带来了极大的便利，不受标准数据同步产品部署的条框限制 再者，它是开源产品，几乎没有成本但是，它的缺点也是显而易见的： 开源工具更多的是提供基础能力，并不具备任务管理，进度跟踪、校验等等一系列的功能，只能使用者自己通过脚本或者表格记录 单机部署，不提供分布式方案，需要通过调度系统解决 Debezium当一个应用程序将数据写入到数据库时，变更会被记录在日志文件中，然后数据库的表才会被更新。对于MySQL来说，日志文件是binlog；对于PostgreSQL来说，是write-ahead-log；而对于MongoDB来说，是op日志。Debezium有针对不同数据库的连接器，所以它能完全理解所有这些日志文件格式的艰巨工作。Debezium可以读取日志文件，并产生一个通用的抽象事件到消息系统中，如Apache Kafka，其中会包含数据的变化。 CanalCanal是阿里开源的一款基于MySQL数据库的binlog增量订阅和消费组件，通过它可以订阅数据库的binlog日志，然后进行一些数据消费，如数据镜像、数据异构、数据索引、缓存更新等。相对于消息队列，通过这种机制可以实现数据的有序化和一致性。 Canal模拟MySQL slave与MySQL master的交互协议，伪装自己是一个MySQL slave，向MySQL master发送dump协议。MySQL master收到dump请求，开始推送binlog增量日志给Canal。Canal收到binlog增量日志后，就可以对这部分日志进行解析，获取主库的结构及数据变更。再发送到存储目的地。 BinlogMySQL日志主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类。其中比较重要的就是二进制日志binlog（归档日志）、事务日志redo log（重做日志）和undo log（回滚日志）。 redo log（重做日志）是InnoDB存储引擎独有的，它让MySQL有了崩溃恢复的能力。当MySQL实例挂了或者宕机了，重启的时候InnoDB存储引擎会使用rede log日志恢复数据，保证事务的持久性和完整性。 binlog（归档日志）是逻辑日志，记录内容是语句的原始逻辑，属于MySQL Server层。所有的存储引擎只要发生了数据更新，都会产生binlog日志。MySQL数据库的数据备份、主备、主主、住从都离不开binlog，需要依赖binlog来同步数据，保证数据一致性。Binlog是记录所有数据库表结构变更以及表数据修改的二进制日志，不会记录SELECT和SHOW这类操作。Binlog日志是以事件形式记录，还包含语句所执行的消耗时间。 想要保证事务的原子性，就需要在发生异常时，对已经执行的操作进行回滚，在MySQL中恢复机制是通过undo log（回滚日志）实现的，所有事务进行的修改都会先被记录到这个回滚日志，然后再执行其他相关的操作。如果执行过程中遇到异常的话，我们直接利用回滚日志中的信息将数据回滚到修改之前的样子。并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。 使用场景 主从复制：在主库中开启Binlog功能，这样主库就可以把Binlog传递给从库，从库拿到Binlog后实现数据恢复达到主从数据一致性。 数据恢复：通过mysqlbinlog工具来恢复数据。 记录模式Binlog文件名默认为“主机名_binlog-序列号”格式，例如oak_binlog-000001，也可以在配置文件中指定名称。文件记录模式有STATEMENT、ROW和MIXED三种，具体含义如下： STATEMENT（statement-based replication, SBR） - 每一条被修改数据的SQL都会记录到master的Binlog中，slave在复制的时候SQL进程会解析成和原来master端执行过的相同的SQL再次执行。简称SQL语句复制。比如执行一条update T set update_time &#x3D; now() where id &#x3D; 1，记录内容如下： 优点：日志量小，减少磁盘IO，提升存储和恢复速度 缺点：在某些情况下会导致主从数据不一致，比如last_insert_id()、now()等函数 ROW（row-based replication, RBR） - 日志中会记录每一行数据被修改的情况，然后在slave端对相同的数据进行修改。同一句SQL，记录内容如下： 优点：能清楚记录每一个行数据的修改细节，能完全实现主从数据同步和数据的恢复 缺点：批量操作，会产生大量的日志，尤其是alter table会让日志暴涨 MIXED（mixed-based replication, MBR） - 以上两种模式的混合使用，一般会使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog，MySQL会根据执行的SQL语句选择写入模式。例如上述SQL如果记录未STATMENT模式会由于now()造成数据不一致，所以会记录未MIXED格式。 写入机制binlog的写入时机为事务执行过程中，先把日志写到binlog cache，事务提交的时候再把binlog cache写到binlog文件中（实际先会写入page cache，然后再由fsync写入binlog文件）。 因为一个事务的binlog不能被拆开，无论这个事务多大，也要确保一次性写入，所以系统会给每个线程分配一块内存作为binlog cache。可以通过binlog_cache_size参数控制单线程binlog_cache大小，如果存储内容超过了这个参数，就要暂存到磁盘。 上图的write，是指把日志写入到文件系统的page cache，并没有把数据持久化硬盘，所以速度比较快。 上图的 fsync才是将数据库持久化到硬盘的操作。 write和fsync的时机可以由参数sync_binlog控制，可以配置成0、1、N(N&gt;1)。 设置成0时：表示每次提交事务都只会write，由系统自行判断什么时候执行fsync。 设置成1时：表示每次提交事务都会执行fsync，就和redo log日志刷盘流程一样。 设置成N时：表示每次提交事务都会write，但是积累N个事务后才fsync。 操作代码1234567891011121314151617181920212223242526272829303132333435363738394041-- binlog状态查看show variables like &#x27;log_bin&#x27;;-- 开启Binlog功能，需要需改my.cnf或my.ini配置文件，在mysqld下面增加log_bin=mysql_bin_log，重启MySQL服务#log-bin=ON #log-bin-basename=mysqlbinlog binlog-format=ROW log-bin=mysqlbinlog-- 执行开启语句set global log_bin=mysqllogbin; -- 使用show binlog event命令show binary logs; //等价于show master logs; show master status; show binlog events; show binlog events in &#x27;mysqlbinlog.000001&#x27;\\G;结果: Log_name: mysql_bin.000001 //此条log存在那个文件中 Pos: 174 //log在bin-log中的开始位置 Event_type: Intvar //log的类型信息 Server_id: 1 //可以查看配置中的server_id,表示log是那个服务器产生 End_log_pos: 202 //log在bin-log中的结束位置 Info: INSERT_ID=2 //log的一些备注信息，可以直观的看出进行了什么操作 -- 可以用mysql自带的工具mysqlbinlogmysqlbinlog &quot;文件名&quot; mysqlbinlog &quot;文件名&quot; &gt; &quot;文件名比如:test.sql&quot; -- 使用binlog恢复数据// 按指定时间恢复 mysqlbinlog --start-datetime=&quot;2020-04-25 18:00:00&quot; --stop-datetime=&quot;2020-04-26 00:00:00&quot; mysqlbinlog.000002 | mysql -uroot -p1234 // 按事件位置号恢复 mysqlbinlog --start-position=154 --stop-position=957 mysqlbinlog.000002 | mysql -uroot -p1234// mysqldump：定期全部备份数据库数据// mysqlbinlog: 可以做增量备份和恢复操作-- 删除binlog文件purge binary logs to &#x27;mysqlbinlog.000001&#x27;; // 删除指定文件 purge binary logs before &#x27;2020-04-28 00:00:00&#x27;; // 删除指定时间之前的文件 reset master; // 清除所有文件// 可以通过设置expire_logs_days参数来启动自动清理功能// 默认值为0表示没启用。设置为1表示超出1天binlog文件会自动删除掉 Redo Log和Binlog的区别 Redo Log是属于InnoDB引擎功能，Binlog是属于MySQL Server自带功能，并且是以二进制文件记录。 Redo Log属于物理日志，记录该数据页更新状态内容，Binlog是逻辑日志，记录更新过程。 Redo Log日志是循环写，日志空间大小是固定，Binlog是追加写入，写完一个写下一个，不会覆盖使用。 Redo Log作为服务器异常宕机后事务数据自动恢复使用，Binlog可以作为主从复制和数据恢复使用。Binlog没有自动crash-safe能力。 crash-safe即在InnoDB 存储引擎中，事务提交过程中任何阶段，MySQL突然奔溃，重启后都能保证事务的完整性，已提交的数据不会丢失，未提交完整的数据会自动进行回滚。这个能力依赖的就是redo log和undo log两个日志 参考文章DataX入门Debezium入门Canal入门","categories":[{"name":"Data","slug":"Data","permalink":"https://dangyoo.github.io/categories/Data/"},{"name":"Integration","slug":"Data/Integration","permalink":"https://dangyoo.github.io/categories/Data/Integration/"},{"name":"CDC","slug":"Data/Integration/CDC","permalink":"https://dangyoo.github.io/categories/Data/Integration/CDC/"}],"tags":[{"name":"知识","slug":"知识","permalink":"https://dangyoo.github.io/tags/%E7%9F%A5%E8%AF%86/"}]}],"categories":[{"name":"Dev Tools","slug":"Dev-Tools","permalink":"https://dangyoo.github.io/categories/Dev-Tools/"},{"name":"Git","slug":"Dev-Tools/Git","permalink":"https://dangyoo.github.io/categories/Dev-Tools/Git/"},{"name":"Data","slug":"Data","permalink":"https://dangyoo.github.io/categories/Data/"},{"name":"Integration","slug":"Data/Integration","permalink":"https://dangyoo.github.io/categories/Data/Integration/"},{"name":"Debezium","slug":"Data/Integration/Debezium","permalink":"https://dangyoo.github.io/categories/Data/Integration/Debezium/"},{"name":"小项目","slug":"小项目","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/"},{"name":"日历订阅","slug":"小项目/日历订阅","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E6%97%A5%E5%8E%86%E8%AE%A2%E9%98%85/"},{"name":"Zookeeper","slug":"Data/Integration/Zookeeper","permalink":"https://dangyoo.github.io/categories/Data/Integration/Zookeeper/"},{"name":"微信公众号","slug":"小项目/微信公众号","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7/"},{"name":"搭梯子","slug":"小项目/搭梯子","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E6%90%AD%E6%A2%AF%E5%AD%90/"},{"name":"低代码平台","slug":"小项目/低代码平台","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E4%BD%8E%E4%BB%A3%E7%A0%81%E5%B9%B3%E5%8F%B0/"},{"name":"日常考据","slug":"日常考据","permalink":"https://dangyoo.github.io/categories/%E6%97%A5%E5%B8%B8%E8%80%83%E6%8D%AE/"},{"name":"博客搭建","slug":"小项目/博客搭建","permalink":"https://dangyoo.github.io/categories/%E5%B0%8F%E9%A1%B9%E7%9B%AE/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"Kafka","slug":"Data/Integration/Kafka","permalink":"https://dangyoo.github.io/categories/Data/Integration/Kafka/"},{"name":"Flume","slug":"Data/Integration/Flume","permalink":"https://dangyoo.github.io/categories/Data/Integration/Flume/"},{"name":"CDC","slug":"Data/Integration/CDC","permalink":"https://dangyoo.github.io/categories/Data/Integration/CDC/"}],"tags":[{"name":"知识","slug":"知识","permalink":"https://dangyoo.github.io/tags/%E7%9F%A5%E8%AF%86/"},{"name":"实践","slug":"实践","permalink":"https://dangyoo.github.io/tags/%E5%AE%9E%E8%B7%B5/"},{"name":"常识","slug":"常识","permalink":"https://dangyoo.github.io/tags/%E5%B8%B8%E8%AF%86/"},{"name":"烹饪","slug":"烹饪","permalink":"https://dangyoo.github.io/tags/%E7%83%B9%E9%A5%AA/"}]}